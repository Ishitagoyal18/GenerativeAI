{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown\n!gdown 1zfJwf1yYgdDl9QrxFZokrdILKvxH9mXr","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-07T20:01:37.904982Z","iopub.execute_input":"2024-08-07T20:01:37.905668Z","iopub.status.idle":"2024-08-07T20:02:15.823662Z","shell.execute_reply.started":"2024-08-07T20:01:37.905637Z","shell.execute_reply":"2024-08-07T20:02:15.822797Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.7.4)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.2.0\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1zfJwf1yYgdDl9QrxFZokrdILKvxH9mXr\nFrom (redirected): https://drive.google.com/uc?id=1zfJwf1yYgdDl9QrxFZokrdILKvxH9mXr&confirm=t&uuid=cc9a8ad3-c2c6-472a-8af7-adee6c18c9b9\nTo: /kaggle/working/CelebAMask-HQ.zip\n100%|███████████████████████████████████████| 3.15G/3.15G [00:18<00:00, 173MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import zipfile\n\nz= zipfile.ZipFile('CelebAMask-HQ.zip')\nz.extractall()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:02:38.243541Z","iopub.execute_input":"2024-08-07T20:02:38.243934Z","iopub.status.idle":"2024-08-07T20:03:49.723389Z","shell.execute_reply.started":"2024-08-07T20:02:38.243905Z","shell.execute_reply":"2024-08-07T20:03:49.722368Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport glob\nimport numpy as np\nfrom tqdm import tqdm\n\nlabel_list = ['skin', 'nose', 'eye_g', 'l_eye', 'r_eye', 'l_brow', 'r_brow', 'l_ear', 'r_ear', 'mouth', 'u_lip', 'l_lip', 'hair', 'hat', 'ear_r', 'neck_l', 'neck', 'cloth']\n\nfolder_base = 'CelebAMask-HQ/CelebAMask-HQ-mask-anno'\nfolder_save = 'CelebAMask-HQ/CelebAMask-HQ-mask'\nimg_num = 30000\n\nif not os.path.exists(folder_save):\n    os.mkdir(folder_save)\n\nfor k in tqdm(range(img_num)):\n    folder_num = k // 2000\n    im_base = np.zeros((512, 512))\n    for idx, label in enumerate(label_list):\n        filename = os.path.join(folder_base, str(folder_num), str(k).rjust(5, '0') + '_' + label + '.png')\n        if os.path.exists(filename):\n            im = cv2.imread(filename)\n            im = im[:, :, 0]\n            im_base[im != 0] = (idx + 1)\n\n    filename_save = os.path.join(folder_save, str(k) + '.png')\n    cv2.imwrite(filename_save, im_base)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:19:33.836964Z","iopub.execute_input":"2024-08-07T20:19:33.837330Z","iopub.status.idle":"2024-08-07T20:33:27.950427Z","shell.execute_reply.started":"2024-08-07T20:19:33.837296Z","shell.execute_reply":"2024-08-07T20:33:27.949564Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"100%|██████████| 30000/30000 [13:54<00:00, 35.97it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom random import randint, choice, shuffle, sample\n\nALL_ATTRIBUTE =  False\nNUM_CAPTION = 10 if ALL_ATTRIBUTE is False else 1\n\nATTRIBUTES = {\n    'IsAttributes': ['Attractive', 'Bald', 'Chubby', 'Young', 'Smiling'],\n    'HasAttributes': ['Eyeglasses', 'Arched_Eyebrows', 'Bags_Under_Eyes', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair',\n               'Blond_Hair', 'Brown_Hair', 'Bushy_Eyebrows', 'Double_Chin', 'Goatee', 'Gray_Hair', 'Straight_Hair',\n               'Sideburns', 'Rosy_Cheeks', 'Receding_Hairline', 'Pointy_Nose', 'Pale_Skin', 'Oval_Face', 'Narrow_Eyes',\n               'Mustache', 'Mouth_Slightly_Open', 'High_Cheekbones', 'Wavy_Hair'],\n    'WearAttributes': ['Wearing_Necktie', 'Wearing_Necklace', 'Wearing_Lipstick', 'Wearing_Hat', 'Wearing_Earrings',\n                'Heavy_Makeup'],\n}\n\ngender = {\n    'female': ['She', 'This woman', 'The woman', 'The person', 'This person'], # in the picture, in the image, the entire face of\n    'male': ['He', 'This man', 'The man', 'The person', 'This person']\n}\n\nIsVerb = [' is ', ' looks ', ' appears to be ']\nWearVerb =[' wears ', ' is wearing ']\nHaveVerb = [' has ', ' is with ']\n\ndef get_subject(img_attribute):\n    '''\n    This function gives options of subject for a given image.\n    '''\n    if img_attribute['Male'] == str(-1):\n        return gender['female']\n    else:\n        return gender['male']\n\ndef get_feature(img_attribute):\n    '''\n    This function gives three categories attributes for a given image. The output for image 29999.jpg is shown below.\n    feature = {'IsAttributes': ['Attractive', 'Smiling'],\n    'HasAttributes': ['Bangs', 'Brown_Hair', 'Rosy_Cheeks', 'Mouth_Slightly_Open', 'High_Cheekbones', 'Wavy_Hair'],\n    'WearAttributes': ['Wearing_Lipstick', 'Wearing_Earrings', 'Heavy_Makeup']}\n    '''\n    feature = {}\n    for attribute, values in ATTRIBUTES.items():\n        feature[attribute] = [value for value in values if img_attribute[value] == str(1)]\n    return feature\n\ndef get_caption(img_attribute, num_caption):\n    '''\n    This function gives a certain numbers of captions for every images in the dataset.\n    '''\n    subject = get_subject(img_attribute)\n    feature = get_feature(img_attribute)\n\n    captions = []\n\n    for _ in range(num_caption):\n\n        if ALL_ATTRIBUTE:\n            IsAttributes = ', '.join(feature['IsAttributes']).lower()[::]\n            HasAttributes = ', '.join(feature['HasAttributes']).replace('_', ' ').lower()[::]\n            WearAttributes = ', '.join(feature['WearAttributes']).replace('Wearing_', '').replace('_', ' ').lower()[::]\n\n            caption = f'The person in the picture is {IsAttributes}. {choice(subject)}{choice(HaveVerb)}{HasAttributes}. {choice(subject)}{choice(WearVerb)}{WearAttributes}.'\n            captions.append(caption)\n\n        else:\n            len_i = len(feature['IsAttributes'])\n            len_h = len(feature['HasAttributes'])\n            len_w = len(feature['WearAttributes'])\n\n            c_i = randint(1, len_i)  if len_i > 1 else len_i\n            c_h = randint(1, len_h)  if len_h > 1 else len_h\n            c_w = randint(1, len_w)  if len_w > 1 else len_w\n\n            IsAttributes = ', '.join(sample(feature['IsAttributes'], c_i)).lower()[::]\n            HasAttributes = ', '.join(sample(feature['HasAttributes'], c_h)).replace('_', ' ').lower()[::]\n            WearAttributes = ', '.join(sample(feature['WearAttributes'], c_w)).replace('Wearing_', '').replace('_', ' ').lower()[::]\n\n            SelectWearAttributes = f'{choice(WearVerb)}{WearAttributes}'\n            SelectHasAttributes = f'{choice(HaveVerb)}{HasAttributes}'\n            SelectIsAttributes = f'{choice(subject)}{choice(IsVerb)}{IsAttributes}'\n\n            caption_format = {\n                '1': f'{SelectIsAttributes}. {choice(subject)}{SelectHasAttributes} and{SelectWearAttributes}.',\n                '2': f'{SelectIsAttributes} and{SelectHasAttributes}. {choice(subject)}{SelectWearAttributes}.',\n                '3': f'This is a {IsAttributes} person. {choice(subject)}{SelectHasAttributes}. {choice(subject)}{SelectWearAttributes}.',\n            }\n\n            random_format = choice(list(caption_format.keys()))\n            caption = caption_format[random_format]\n            captions.append(caption)\n\n    return captions\n\n\nif __name__ == \"__main__\":\n\n    anno_path = '/kaggle/working/CelebAMask-HQ/CelebAMask-HQ-attribute-anno.txt'\n    save_path = '/kaggle/working/CelebAMask-HQ/celeba_caption'\n\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n\n    with open(anno_path, 'r') as f:\n        lines = f.readlines()\n        num_images = int(lines[0])\n        attributes = lines[1].split()\n        image_attributes = {}\n        for i in range(num_images):\n            image_id, *attr_values = lines[i+2].split()\n            image_attributes[image_id] = dict(zip(attributes, attr_values))\n\n    for num in range (0, num_images):\n        captions = get_caption(image_attributes['{}.jpg'.format(num)], NUM_CAPTION)\n\n        with open('{}/{}.txt'.format(save_path, str(num)), \"w\") as f:\n            f.write(\"\\n\".join(captions))\n\n    print ('all done!')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:34:33.018634Z","iopub.execute_input":"2024-08-07T20:34:33.018997Z","iopub.status.idle":"2024-08-07T20:34:44.283556Z","shell.execute_reply.started":"2024-08-07T20:34:33.018969Z","shell.execute_reply":"2024-08-07T20:34:44.282545Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"all done!\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nimport glob\nimport os\nimport torch\n\n\ndef load_latents(latent_path):\n    latent_maps = {}\n    for fname in glob.glob(os.path.join(latent_path, '*.pkl')):\n        s = pickle.load(open(fname, 'rb'))\n        for k, v in s.items():\n            latent_maps[k] = v[0]\n    return latent_maps\n\n\ndef drop_text_condition(text_embed, im, empty_text_embed, text_drop_prob):\n    if text_drop_prob > 0:\n        text_drop_mask = torch.zeros((im.shape[0]), device=im.device).float().uniform_(0,\n                                                                                       1) < text_drop_prob\n        assert empty_text_embed is not None, (\"Text Conditioning required as well as\"\n                                        \" text dropping but empty text representation not created\")\n        text_embed[text_drop_mask, :, :] = empty_text_embed[0]\n    return text_embed\n\n\ndef drop_image_condition(image_condition, im, im_drop_prob):\n    if im_drop_prob > 0:\n        im_drop_mask = torch.zeros((im.shape[0], 1, 1, 1), device=im.device).float().uniform_(0,\n                                                                                        1) > im_drop_prob\n        return image_condition * im_drop_mask\n    else:\n        return image_condition\n\n\ndef drop_class_condition(class_condition, class_drop_prob, im):\n    if class_drop_prob > 0:\n        class_drop_mask = torch.zeros((im.shape[0], 1), device=im.device).float().uniform_(0,\n                                                                                           1) > class_drop_prob\n        return class_condition * class_drop_mask\n    else:\n        return class_condition","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:34:51.428439Z","iopub.execute_input":"2024-08-07T20:34:51.429221Z","iopub.status.idle":"2024-08-07T20:34:57.064505Z","shell.execute_reply.started":"2024-08-07T20:34:51.429191Z","shell.execute_reply":"2024-08-07T20:34:57.063714Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import glob\nimport os\nimport random\nimport torch\nimport torchvision\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.utils.data.dataset import Dataset\nimport torch\nimport torch.nn as nn\n\n\nclass CelebDataset(Dataset):\n    def __init__(self, split, im_path, im_size=256, im_channels=3, im_ext='jpg',\n                 use_latents=False, latent_path=None, condition_config=None):\n        self.split = split\n        self.im_size = im_size\n        self.im_channels = im_channels\n        self.im_ext = im_ext\n        self.im_path = im_path\n        self.latent_maps = None\n        self.use_latents = False\n\n        self.condition_types = [] if condition_config is None else condition_config['condition_types']\n\n        self.idx_to_cls_map = {}\n        self.cls_to_idx_map ={}\n\n        if 'image' in self.condition_types:\n            self.mask_channels = condition_config['image_condition_config']['image_condition_input_channels']\n            self.mask_h = condition_config['image_condition_config']['image_condition_h']\n            self.mask_w = condition_config['image_condition_config']['image_condition_w']\n\n        self.images, self.texts, self.masks = self.load_images(im_path)\n\n        if use_latents and latent_path is not None:\n            latent_maps = load_latents(latent_path)\n            if len(latent_maps) == len(self.images):\n                self.use_latents = True\n                self.latent_maps = latent_maps\n                print('Found {} latents'.format(len(self.latent_maps)))\n            else:\n                print('Latents not found')\n\n    def load_images(self, im_path):\n        assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n        ims = []\n        fnames = glob.glob(os.path.join(im_path, 'CelebA-HQ-img/*.{}'.format('png')))\n        fnames += glob.glob(os.path.join(im_path, 'CelebA-HQ-img/*.{}'.format('jpg')))\n        fnames += glob.glob(os.path.join(im_path, 'CelebA-HQ-img/*.{}'.format('jpeg')))\n        texts = []\n        masks = []\n\n        if 'image' in self.condition_types:\n            label_list = ['skin', 'nose', 'eye_g', 'l_eye', 'r_eye', 'l_brow', 'r_brow', 'l_ear', 'r_ear', 'mouth',\n                          'u_lip', 'l_lip', 'hair', 'hat', 'ear_r', 'neck_l', 'neck', 'cloth']\n            self.idx_to_cls_map = {idx: label_list[idx] for idx in range(len(label_list))}\n            self.cls_to_idx_map = {label_list[idx]: idx for idx in range(len(label_list))}\n\n        for fname in tqdm(fnames):\n            ims.append(fname)\n\n            if 'text' in self.condition_types:\n                im_name = os.path.split(fname)[1].split('.')[0]\n                captions_im = []\n                caption_path = os.path.join(im_path, 'celeba_caption/{}.txt'.format(im_name))\n                if os.path.exists(caption_path):\n                  with open(os.path.join(im_path, 'celeba_caption/{}.txt'.format(im_name))) as f:\n                    for line in f.readlines():\n                        captions_im.append(line.strip())\n                  texts.append(captions_im)\n                else:\n                   print(f\"Warning: Caption file {caption_path} not found.\")\n\n\n            if 'image' in self.condition_types:\n                im_name = int(os.path.split(fname)[1].split('.')[0])\n                masks.append(os.path.join(im_path, 'CelebAMask-HQ-mask', '{}.png'.format(im_name)))\n        if 'text' in self.condition_types:\n            assert len(texts) == len(ims), \"Condition Type Text but could not find captions for all images\"\n        if 'image' in self.condition_types:\n            assert len(masks) == len(ims), \"Condition Type Image but could not find masks for all images\"\n        print('Found {} images'.format(len(ims)))\n        print('Found {} masks'.format(len(masks)))\n        print('Found {} captions'.format(len(texts)))\n        return ims, texts, masks\n\n    def get_mask(self, index):\n        mask_im = Image.open(self.masks[index])\n        mask_im = np.array(mask_im)\n        im_base = np.zeros((self.mask_h, self.mask_w, self.mask_channels))\n        for orig_idx in range(len(self.idx_to_cls_map)):\n            im_base[mask_im == (orig_idx+1), orig_idx] = 1\n        mask = torch.from_numpy(im_base).permute(2, 0, 1).float()\n        return mask\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        cond_inputs = {}\n        if 'text' in self.condition_types:\n            cond_inputs['text'] = random.sample(self.texts[index], k=1)[0]\n        if 'image' in self.condition_types:\n            mask = self.get_mask(index)\n            cond_inputs['image'] = mask\n        if self.use_latents:\n            latent = self.latent_maps[self.images[index]]\n            if len(self.condition_types) == 0:\n                return latent\n            else:\n                return latent, cond_inputs\n        else:\n            im = Image.open(self.images[index])\n            im_tensor = torchvision.transforms.Compose([\n                torchvision.transforms.Resize(self.im_size),\n                torchvision.transforms.CenterCrop(self.im_size),\n                torchvision.transforms.ToTensor(),\n            ])(im)\n            im.close()\n\n            im_tensor = (2 * im_tensor) - 1\n            if len(self.condition_types) == 0:\n                return im_tensor\n            else:\n                return im_tensor, cond_inputs\n\ndef get_time_embedding(time_steps, temb_dim):\n    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n\n    factor = 10000 ** ((torch.arange(\n        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n    )\n\n    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n    return t_emb\n\n\nclass DownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, t_emb_dim,\n                 down_sample, num_heads, num_layers, attn, norm_channels, cross_attn=False, context_dim=None):\n        super().__init__()\n        self.num_layers = num_layers\n        self.down_sample = down_sample\n        self.attn = attn\n        self.context_dim = context_dim\n        self.cross_attn = cross_attn\n        self.t_emb_dim = t_emb_dim\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n                              kernel_size=3, stride=1, padding=1),\n                )\n                for i in range(num_layers)\n            ]\n        )\n        if self.t_emb_dim is not None:\n            self.t_emb_layers = nn.ModuleList([\n                nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(self.t_emb_dim, out_channels)\n                )\n                for _ in range(num_layers)\n            ])\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels,\n                              kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers)\n            ]\n        )\n\n        if self.attn:\n            self.attention_norms = nn.ModuleList(\n                [nn.GroupNorm(norm_channels, out_channels)\n                 for _ in range(num_layers)]\n            )\n\n            self.attentions = nn.ModuleList(\n                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                 for _ in range(num_layers)]\n            )\n\n        if self.cross_attn:\n            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n            self.cross_attention_norms = nn.ModuleList(\n                [nn.GroupNorm(norm_channels, out_channels)\n                 for _ in range(num_layers)]\n            )\n            self.cross_attentions = nn.ModuleList(\n                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                 for _ in range(num_layers)]\n            )\n            self.context_proj = nn.ModuleList(\n                [nn.Linear(context_dim, out_channels)\n                 for _ in range(num_layers)]\n            )\n\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers)\n            ]\n        )\n        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n                                          4, 2, 1) if self.down_sample else nn.Identity()\n\n    def forward(self, x, t_emb=None, context=None):\n        out = x\n        for i in range(self.num_layers):\n            resnet_input = out\n            out = self.resnet_conv_first[i](out)\n            if self.t_emb_dim is not None:\n                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i](out)\n            out = out + self.residual_input_conv[i](resnet_input)\n\n            if self.attn:\n                batch_size, channels, h, w = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w)\n                in_attn = self.attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n                out = out + out_attn\n\n            if self.cross_attn:\n                assert context is not None, \"context cannot be None if cross attention layers are used\"\n                batch_size, channels, h, w = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w)\n                in_attn = self.cross_attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n                context_proj = self.context_proj[i](context)\n                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n                out = out + out_attn\n\n        # Downsample\n        out = self.down_sample_conv(out)\n        return out\n\n\nclass MidBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads, num_layers, norm_channels, cross_attn=None, context_dim=None):\n        super().__init__()\n        self.num_layers = num_layers\n        self.t_emb_dim = t_emb_dim\n        self.context_dim = context_dim\n        self.cross_attn = cross_attn\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n                              padding=1),\n                )\n                for i in range(num_layers + 1)\n            ]\n        )\n\n        if self.t_emb_dim is not None:\n            self.t_emb_layers = nn.ModuleList([\n                nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(t_emb_dim, out_channels)\n                )\n                for _ in range(num_layers + 1)\n            ])\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers + 1)\n            ]\n        )\n\n        self.attention_norms = nn.ModuleList(\n            [nn.GroupNorm(norm_channels, out_channels)\n             for _ in range(num_layers)]\n        )\n\n        self.attentions = nn.ModuleList(\n            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n             for _ in range(num_layers)]\n        )\n        if self.cross_attn:\n            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n            self.cross_attention_norms = nn.ModuleList(\n                [nn.GroupNorm(norm_channels, out_channels)\n                 for _ in range(num_layers)]\n            )\n            self.cross_attentions = nn.ModuleList(\n                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                 for _ in range(num_layers)]\n            )\n            self.context_proj = nn.ModuleList(\n                [nn.Linear(context_dim, out_channels)\n                 for _ in range(num_layers)]\n            )\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers + 1)\n            ]\n        )\n\n    def forward(self, x, t_emb=None, context=None):\n        out = x\n\n        resnet_input = out\n        out = self.resnet_conv_first[0](out)\n        if self.t_emb_dim is not None:\n            out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n        out = self.resnet_conv_second[0](out)\n        out = out + self.residual_input_conv[0](resnet_input)\n\n        for i in range(self.num_layers):\n            batch_size, channels, h, w = out.shape\n            in_attn = out.reshape(batch_size, channels, h * w)\n            in_attn = self.attention_norms[i](in_attn)\n            in_attn = in_attn.transpose(1, 2)\n            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n            out = out + out_attn\n\n            if self.cross_attn:\n                assert context is not None, \"context cannot be None if cross attention layers are used\"\n                batch_size, channels, h, w = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w)\n                in_attn = self.cross_attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n                context_proj = self.context_proj[i](context)\n                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n                out = out + out_attn\n\n\n            resnet_input = out\n            out = self.resnet_conv_first[i + 1](out)\n            if self.t_emb_dim is not None:\n                out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i + 1](out)\n            out = out + self.residual_input_conv[i + 1](resnet_input)\n\n        return out\n\n\nclass UpBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, t_emb_dim,\n                 up_sample, num_heads, num_layers, attn, norm_channels):\n        super().__init__()\n        self.num_layers = num_layers\n        self.up_sample = up_sample\n        self.t_emb_dim = t_emb_dim\n        self.attn = attn\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n                              padding=1),\n                )\n                for i in range(num_layers)\n            ]\n        )\n\n        if self.t_emb_dim is not None:\n            self.t_emb_layers = nn.ModuleList([\n                nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(t_emb_dim, out_channels)\n                )\n                for _ in range(num_layers)\n            ])\n\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        if self.attn:\n            self.attention_norms = nn.ModuleList(\n                [\n                    nn.GroupNorm(norm_channels, out_channels)\n                    for _ in range(num_layers)\n                ]\n            )\n\n            self.attentions = nn.ModuleList(\n                [\n                    nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                    for _ in range(num_layers)\n                ]\n            )\n\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers)\n            ]\n        )\n        self.up_sample_conv = nn.ConvTranspose2d(in_channels, in_channels,\n                                                 4, 2, 1) \\\n            if self.up_sample else nn.Identity()\n\n    def forward(self, x, out_down=None, t_emb=None):\n        x = self.up_sample_conv(x)\n\n        if out_down is not None:\n            x = torch.cat([x, out_down], dim=1)\n\n        out = x\n        for i in range(self.num_layers):\n            resnet_input = out\n            out = self.resnet_conv_first[i](out)\n            if self.t_emb_dim is not None:\n                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i](out)\n            out = out + self.residual_input_conv[i](resnet_input)\n\n            if self.attn:\n                batch_size, channels, h, w = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w)\n                in_attn = self.attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n                out = out + out_attn\n        return out\n\n\nclass UpBlockUnet(nn.Module):\n    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample,\n                 num_heads, num_layers, norm_channels, cross_attn=False, context_dim=None):\n        super().__init__()\n        self.num_layers = num_layers\n        self.up_sample = up_sample\n        self.t_emb_dim = t_emb_dim\n        self.cross_attn = cross_attn\n        self.context_dim = context_dim\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n                              padding=1),\n                )\n                for i in range(num_layers)\n            ]\n        )\n\n        if self.t_emb_dim is not None:\n            self.t_emb_layers = nn.ModuleList([\n                nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(t_emb_dim, out_channels)\n                )\n                for _ in range(num_layers)\n            ])\n\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers)\n            ]\n        )\n\n        self.attention_norms = nn.ModuleList(\n            [\n                nn.GroupNorm(norm_channels, out_channels)\n                for _ in range(num_layers)\n            ]\n        )\n\n        self.attentions = nn.ModuleList(\n            [\n                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                for _ in range(num_layers)\n            ]\n        )\n\n        if self.cross_attn:\n            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n            self.cross_attention_norms = nn.ModuleList(\n                [nn.GroupNorm(norm_channels, out_channels)\n                 for _ in range(num_layers)]\n            )\n            self.cross_attentions = nn.ModuleList(\n                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                 for _ in range(num_layers)]\n            )\n            self.context_proj = nn.ModuleList(\n                [nn.Linear(context_dim, out_channels)\n                 for _ in range(num_layers)]\n            )\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers)\n            ]\n        )\n        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n                                                 4, 2, 1) \\\n            if self.up_sample else nn.Identity()\n\n    def forward(self, x, out_down=None, t_emb=None, context=None):\n        x = self.up_sample_conv(x)\n        if out_down is not None:\n            x = torch.cat([x, out_down], dim=1)\n\n        out = x\n        for i in range(self.num_layers):\n            resnet_input = out\n            out = self.resnet_conv_first[i](out)\n            if self.t_emb_dim is not None:\n                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i](out)\n            out = out + self.residual_input_conv[i](resnet_input)\n            batch_size, channels, h, w = out.shape\n            in_attn = out.reshape(batch_size, channels, h * w)\n            in_attn = self.attention_norms[i](in_attn)\n            in_attn = in_attn.transpose(1, 2)\n            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n            out = out + out_attn\n            if self.cross_attn:\n                assert context is not None, \"context cannot be None if cross attention layers are used\"\n                batch_size, channels, h, w = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w)\n                in_attn = self.cross_attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                assert len(context.shape) == 3, \\\n                    \"Context shape does not match B,_,CONTEXT_DIM\"\n                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim,\\\n                    \"Context shape does not match B,_,CONTEXT_DIM\"\n                context_proj = self.context_proj[i](context)\n                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n                out = out + out_attn\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:35:22.173731Z","iopub.execute_input":"2024-08-07T20:35:22.174237Z","iopub.status.idle":"2024-08-07T20:35:22.269714Z","shell.execute_reply.started":"2024-08-07T20:35:22.174205Z","shell.execute_reply":"2024-08-07T20:35:22.268618Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def validate_class_config(condition_config):\n    assert 'class_condition_config' in condition_config, \\\n        \"Class conditioning desired but class condition config missing\"\n    assert 'num_classes' in condition_config['class_condition_config'], \\\n        \"num_class missing in class condition config\"\n\n\ndef validate_text_config(condition_config):\n    assert 'text_condition_config' in condition_config, \\\n        \"Text conditioning desired but text condition config missing\"\n    assert 'text_embed_dim' in condition_config['text_condition_config'], \\\n        \"text_embed_dim missing in text condition config\"\n\n\ndef validate_image_config(condition_config):\n    assert 'image_condition_config' in condition_config, \\\n        \"Image conditioning desired but image condition config missing\"\n    assert 'image_condition_input_channels' in condition_config['image_condition_config'], \\\n        \"image_condition_input_channels missing in image condition config\"\n    assert 'image_condition_output_channels' in condition_config['image_condition_config'], \\\n        \"image_condition_output_channels missing in image condition config\"\n\n\ndef validate_image_conditional_input(cond_input, x):\n    assert 'image' in cond_input, \\\n        \"Model initialized with image conditioning but cond_input has no image information\"\n    assert cond_input['image'].shape[0] == x.shape[0], \\\n        \"Batch size mismatch of image condition and input\"\n    assert cond_input['image'].shape[2] % x.shape[2] == 0, \\\n        \"Height/Width of image condition must be divisible by latent input\"\n\n\ndef validate_class_conditional_input(cond_input, x, num_classes):\n    assert 'class' in cond_input, \\\n        \"Model initialized with class conditioning but cond_input has no class information\"\n    assert cond_input['class'].shape == (x.shape[0], num_classes), \\\n        \"Shape of class condition input must match (Batch Size, )\"\ndef get_config_value(config, key, default_value):\n    return config[key] if key in config else default_value","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:35:25.091645Z","iopub.execute_input":"2024-08-07T20:35:25.092429Z","iopub.status.idle":"2024-08-07T20:35:25.101611Z","shell.execute_reply.started":"2024-08-07T20:35:25.092398Z","shell.execute_reply":"2024-08-07T20:35:25.100702Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"!pip install einops\nimport torch\nfrom einops import einsum\nimport torch.nn as nn\n\n\nclass Unet(nn.Module):\n    def __init__(self, im_channels, model_config):\n        super().__init__()\n        self.down_channels = model_config['down_channels']\n        self.mid_channels = model_config['mid_channels']\n        self.t_emb_dim = model_config['time_emb_dim']\n        self.down_sample = model_config['down_sample']\n        self.num_down_layers = model_config['num_down_layers']\n        self.num_mid_layers = model_config['num_mid_layers']\n        self.num_up_layers = model_config['num_up_layers']\n        self.attns = model_config['attn_down']\n        self.norm_channels = model_config['norm_channels']\n        self.num_heads = model_config['num_heads']\n        self.conv_out_channels = model_config['conv_out_channels']\n\n        assert self.mid_channels[0] == self.down_channels[-1]\n        assert self.mid_channels[-1] == self.down_channels[-2]\n        assert len(self.down_sample) == len(self.down_channels) - 1\n        assert len(self.attns) == len(self.down_channels) - 1\n\n        self.class_cond = False\n        self.text_cond = False\n        self.image_cond = False\n        self.text_embed_dim = None\n        self.condition_config = get_config_value(model_config, 'condition_config', None)\n        if self.condition_config is not None:\n            assert 'condition_types' in self.condition_config, 'Condition Type not provided in model config'\n            condition_types = self.condition_config['condition_types']\n            if 'class' in condition_types:\n                validate_class_config(self.condition_config)\n                self.class_cond = True\n                self.num_classes = self.condition_config['class_condition_config']['num_classes']\n            if 'text' in condition_types:\n                validate_text_config(self.condition_config)\n                self.text_cond = True\n                self.text_embed_dim = self.condition_config['text_condition_config']['text_embed_dim']\n            if 'image' in condition_types:\n                self.image_cond = True\n                self.im_cond_input_ch = self.condition_config['image_condition_config'][\n                    'image_condition_input_channels']\n                self.im_cond_output_ch = self.condition_config['image_condition_config'][\n                    'image_condition_output_channels']\n        if self.class_cond:\n            self.class_emb = nn.Embedding(self.num_classes,\n                                          self.t_emb_dim)\n\n        if self.image_cond:\n            self.cond_conv_in = nn.Conv2d(in_channels=self.im_cond_input_ch,\n                                          out_channels=self.im_cond_output_ch,\n                                          kernel_size=1,\n                                          bias=False)\n            self.conv_in_concat = nn.Conv2d(im_channels + self.im_cond_output_ch,\n                                            self.down_channels[0], kernel_size=3, padding=1)\n        else:\n            self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=1)\n        self.cond = self.text_cond or self.image_cond or self.class_cond\n        self.t_proj = nn.Sequential(\n            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n            nn.SiLU(),\n            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n        )\n\n        self.up_sample = list(reversed(self.down_sample))\n        self.downs = nn.ModuleList([])\n\n        for i in range(len(self.down_channels) - 1):\n            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i + 1], self.t_emb_dim,\n                                        down_sample=self.down_sample[i],\n                                        num_heads=self.num_heads,\n                                        num_layers=self.num_down_layers,\n                                        attn=self.attns[i], norm_channels=self.norm_channels,\n                                        cross_attn=self.text_cond,\n                                        context_dim=self.text_embed_dim))\n\n        self.mids = nn.ModuleList([])\n        for i in range(len(self.mid_channels) - 1):\n            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1], self.t_emb_dim,\n                                      num_heads=self.num_heads,\n                                      num_layers=self.num_mid_layers,\n                                      norm_channels=self.norm_channels,\n                                      cross_attn=self.text_cond,\n                                      context_dim=self.text_embed_dim))\n\n        self.ups = nn.ModuleList([])\n        for i in reversed(range(len(self.down_channels) - 1)):\n            self.ups.append(\n                UpBlockUnet(self.down_channels[i] * 2, self.down_channels[i - 1] if i != 0 else self.conv_out_channels,\n                            self.t_emb_dim, up_sample=self.down_sample[i],\n                            num_heads=self.num_heads,\n                            num_layers=self.num_up_layers,\n                            norm_channels=self.norm_channels,\n                            cross_attn=self.text_cond,\n                            context_dim=self.text_embed_dim))\n\n        self.norm_out = nn.GroupNorm(self.norm_channels, self.conv_out_channels)\n        self.conv_out = nn.Conv2d(self.conv_out_channels, im_channels, kernel_size=3, padding=1)\n\n    def forward(self, x, t, cond_input=None):\n        if self.cond:\n            assert cond_input is not None, \\\n                \"Model initialized with conditioning so cond_input cannot be None\"\n        if self.image_cond:\n            validate_image_conditional_input(cond_input, x)\n            im_cond = cond_input['image']\n            im_cond = torch.nn.functional.interpolate(im_cond, size=x.shape[-2:])\n            im_cond = self.cond_conv_in(im_cond)\n            assert im_cond.shape[-2:] == x.shape[-2:]\n            x = torch.cat([x, im_cond], dim=1)\n            out = self.conv_in_concat(x)\n        else:\n            out = self.conv_in(x)\n\n        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n        t_emb = self.t_proj(t_emb)\n\n        if self.class_cond:\n            validate_class_conditional_input(cond_input, x, self.num_classes)\n            class_embed = einsum(cond_input['class'].float(), self.class_emb.weight, 'b n, n d -> b d')\n            t_emb += class_embed\n\n        context_hidden_states = None\n        if self.text_cond:\n            assert 'text' in cond_input, \\\n                \"Model initialized with text conditioning but cond_input has no text information\"\n            context_hidden_states = cond_input['text']\n        down_outs = []\n\n        for idx, down in enumerate(self.downs):\n            down_outs.append(out)\n            out = down(out, t_emb, context_hidden_states)\n\n        for mid in self.mids:\n            out = mid(out, t_emb, context_hidden_states)\n\n        for up in self.ups:\n            down_out = down_outs.pop()\n            out = up(out, down_out, t_emb, context_hidden_states)\n        out = self.norm_out(out)\n        out = nn.SiLU()(out)\n        out = self.conv_out(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:41:39.421303Z","iopub.execute_input":"2024-08-07T20:41:39.421659Z","iopub.status.idle":"2024-08-07T20:41:51.547031Z","shell.execute_reply.started":"2024-08-07T20:41:39.421632Z","shell.execute_reply":"2024-08-07T20:41:51.546082Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (0.8.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown 1vMD_veh4424mbQ8oPHOFZtNlixtXfLBL\n!gdown 1Afe1u68SVpTLzBbSI3U0_5z7sB-1LVLg","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:48:04.988313Z","iopub.execute_input":"2024-08-07T20:48:04.988743Z","iopub.status.idle":"2024-08-07T20:48:14.213165Z","shell.execute_reply.started":"2024-08-07T20:48:04.988711Z","shell.execute_reply":"2024-08-07T20:48:14.212078Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/download.py:33: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  soup = bs4.BeautifulSoup(line, features=\"html.parser\")\nFailed to retrieve file url:\n\n\tCannot retrieve the public link of the file. You may need to change\n\tthe permission to 'Anyone with the link', or have had many accesses.\n\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1vMD_veh4424mbQ8oPHOFZtNlixtXfLBL\n\nbut Gdown can't. Please check connections and permissions.\nDownloading...\nFrom: https://drive.google.com/uc?id=1Afe1u68SVpTLzBbSI3U0_5z7sB-1LVLg\nTo: /kaggle/working/celebhq_text_cond.yaml\n100%|██████████████████████████████████████| 1.76k/1.76k [00:00<00:00, 8.99MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown 1nJEbo9Q7Jd7iOEzBZSu89iz8PQMyHiaz","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:50:42.800024Z","iopub.execute_input":"2024-08-07T20:50:42.800405Z","iopub.status.idle":"2024-08-07T20:50:47.542487Z","shell.execute_reply.started":"2024-08-07T20:50:42.800375Z","shell.execute_reply":"2024-08-07T20:50:47.541524Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1nJEbo9Q7Jd7iOEzBZSu89iz8PQMyHiaz\nTo: /kaggle/working/weights.zip\n100%|██████████████████████████████████████| 6.26k/6.26k [00:00<00:00, 25.8MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"k= zipfile.ZipFile('weights.zip')\nk.extractall()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:51:23.055099Z","iopub.execute_input":"2024-08-07T20:51:23.055810Z","iopub.status.idle":"2024-08-07T20:51:23.061688Z","shell.execute_reply.started":"2024-08-07T20:51:23.055777Z","shell.execute_reply":"2024-08-07T20:51:23.060831Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass VQVAE(nn.Module):\n    def __init__(self, im_channels, model_config):\n        super().__init__()\n        self.down_channels = model_config['down_channels']\n        self.mid_channels = model_config['mid_channels']\n        self.down_sample = model_config['down_sample']\n        self.num_down_layers = model_config['num_down_layers']\n        self.num_mid_layers = model_config['num_mid_layers']\n        self.num_up_layers = model_config['num_up_layers']\n\n        self.attns = model_config['attn_down']\n\n        self.z_channels = model_config['z_channels']\n        self.codebook_size = model_config['codebook_size']\n        self.norm_channels = model_config['norm_channels']\n        self.num_heads = model_config['num_heads']\n\n        assert self.mid_channels[0] == self.down_channels[-1]\n        assert self.mid_channels[-1] == self.down_channels[-1]\n        assert len(self.down_sample) == len(self.down_channels) - 1\n        assert len(self.attns) == len(self.down_channels) - 1\n\n        self.up_sample = list(reversed(self.down_sample))\n\n        self.encoder_conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1))\n\n        self.encoder_layers = nn.ModuleList([])\n        for i in range(len(self.down_channels) - 1):\n            self.encoder_layers.append(DownBlock(self.down_channels[i], self.down_channels[i + 1],\n                                                 t_emb_dim=None, down_sample=self.down_sample[i],\n                                                 num_heads=self.num_heads,\n                                                 num_layers=self.num_down_layers,\n                                                 attn=self.attns[i],\n                                                 norm_channels=self.norm_channels))\n\n        self.encoder_mids = nn.ModuleList([])\n        for i in range(len(self.mid_channels) - 1):\n            self.encoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1],\n                                              t_emb_dim=None,\n                                              num_heads=self.num_heads,\n                                              num_layers=self.num_mid_layers,\n                                              norm_channels=self.norm_channels))\n\n        self.encoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[-1])\n        self.encoder_conv_out = nn.Conv2d(self.down_channels[-1], self.z_channels, kernel_size=3, padding=1)\n\n        self.pre_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size=1)\n\n        self.embedding = nn.Embedding(self.codebook_size, self.z_channels)\n\n        self.post_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size=1)\n        self.decoder_conv_in = nn.Conv2d(self.z_channels, self.mid_channels[-1], kernel_size=3, padding=(1, 1))\n\n        self.decoder_mids = nn.ModuleList([])\n        for i in reversed(range(1, len(self.mid_channels))):\n            self.decoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i - 1],\n                                              t_emb_dim=None,\n                                              num_heads=self.num_heads,\n                                              num_layers=self.num_mid_layers,\n                                              norm_channels=self.norm_channels))\n\n        self.decoder_layers = nn.ModuleList([])\n        for i in reversed(range(1, len(self.down_channels))):\n            self.decoder_layers.append(UpBlock(self.down_channels[i], self.down_channels[i - 1],\n                                               t_emb_dim=None, up_sample=self.down_sample[i - 1],\n                                               num_heads=self.num_heads,\n                                               num_layers=self.num_up_layers,\n                                               attn=self.attns[i-1],\n                                               norm_channels=self.norm_channels))\n\n        self.decoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[0])\n        self.decoder_conv_out = nn.Conv2d(self.down_channels[0], im_channels, kernel_size=3, padding=1)\n\n    def quantize(self, x):\n        B, C, H, W = x.shape\n\n        x = x.permute(0, 2, 3, 1)\n\n        x = x.reshape(x.size(0), -1, x.size(-1))\n\n        dist = torch.cdist(x, self.embedding.weight[None, :].repeat((x.size(0), 1, 1)))\n        min_encoding_indices = torch.argmin(dist, dim=-1)\n\n        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1))\n\n        x = x.reshape((-1, x.size(-1)))\n        commmitment_loss = torch.mean((quant_out.detach() - x) ** 2)\n        codebook_loss = torch.mean((quant_out - x.detach()) ** 2)\n        quantize_losses = {\n            'codebook_loss': codebook_loss,\n            'commitment_loss': commmitment_loss\n        }\n        quant_out = x + (quant_out - x).detach()\n\n        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2)\n        min_encoding_indices = min_encoding_indices.reshape((-1, quant_out.size(-2), quant_out.size(-1)))\n        return quant_out, quantize_losses, min_encoding_indices\n\n    def encode(self, x):\n        out = self.encoder_conv_in(x)\n        for idx, down in enumerate(self.encoder_layers):\n            out = down(out)\n        for mid in self.encoder_mids:\n            out = mid(out)\n        out = self.encoder_norm_out(out)\n        out = nn.SiLU()(out)\n        out = self.encoder_conv_out(out)\n        out = self.pre_quant_conv(out)\n        out, quant_losses, _ = self.quantize(out)\n        return out, quant_losses\n\n    def decode(self, z):\n        out = z\n        out = self.post_quant_conv(out)\n        out = self.decoder_conv_in(out)\n        for mid in self.decoder_mids:\n            out = mid(out)\n        for idx, up in enumerate(self.decoder_layers):\n            out = up(out)\n\n        out = self.decoder_norm_out(out)\n        out = nn.SiLU()(out)\n        out = self.decoder_conv_out(out)\n        return out\n\n    def forward(self, x):\n        z, quant_losses = self.encode(x)\n        out = self.decode(z)\n        return out, z, quant_losses","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:51:44.266503Z","iopub.execute_input":"2024-08-07T20:51:44.266908Z","iopub.status.idle":"2024-08-07T20:51:44.297274Z","shell.execute_reply.started":"2024-08-07T20:51:44.266879Z","shell.execute_reply":"2024-08-07T20:51:44.296193Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\n\n\nclass LinearNoiseScheduler:\n    def __init__(self, num_timesteps, beta_start, beta_end):\n        self.num_timesteps = num_timesteps\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n        self.betas = (\n                torch.linspace(beta_start ** 0.5, beta_end ** 0.5, num_timesteps) ** 2\n        )\n        self.alphas = 1. - self.betas\n        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n\n    def add_noise(self, original, noise, t):\n        original_shape = original.shape\n        batch_size = original_shape[0]\n\n        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n\n        for _ in range(len(original_shape) - 1):\n            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n        for _ in range(len(original_shape) - 1):\n            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n\n        # Apply and Return Forward process equation\n        return (sqrt_alpha_cum_prod.to(original.device) * original\n                + sqrt_one_minus_alpha_cum_prod.to(original.device) * noise)\n\n    def sample_prev_timestep(self, xt, noise_pred, t):\n        x0 = ((xt - (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t] * noise_pred)) /\n              torch.sqrt(self.alpha_cum_prod.to(xt.device)[t]))\n        x0 = torch.clamp(x0, -1., 1.)\n\n        mean = xt - ((self.betas.to(xt.device)[t]) * noise_pred) / (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t])\n        mean = mean / torch.sqrt(self.alphas.to(xt.device)[t])\n\n        if t == 0:\n            return mean, x0\n        else:\n            variance = (1 - self.alpha_cum_prod.to(xt.device)[t - 1]) / (1.0 - self.alpha_cum_prod.to(xt.device)[t])\n            variance = variance * self.betas.to(xt.device)[t]\n            sigma = variance ** 0.5\n            z = torch.randn(xt.shape).to(xt.device)\n\n            return mean + sigma * z, x0","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:51:47.522687Z","iopub.execute_input":"2024-08-07T20:51:47.523295Z","iopub.status.idle":"2024-08-07T20:51:47.536876Z","shell.execute_reply.started":"2024-08-07T20:51:47.523265Z","shell.execute_reply":"2024-08-07T20:51:47.535972Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import DistilBertModel, DistilBertTokenizer, CLIPTokenizer, CLIPTextModel\n\n\ndef get_tokenizer_and_model(model_type, device, eval_mode=True):\n    assert model_type in ('bert', 'clip'), \"Text model can only be one of clip or bert\"\n    if model_type == 'bert':\n        text_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n        text_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n    else:\n        text_tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch16')\n        text_model = CLIPTextModel.from_pretrained('openai/clip-vit-base-patch16').to(device)\n    if eval_mode:\n        text_model.eval()\n    return text_tokenizer, text_model\n\n\ndef get_text_representation(text, text_tokenizer, text_model, device,\n                            truncation=True,\n                            padding='max_length',\n                            max_length=77):\n    token_output = text_tokenizer(text,\n                                  truncation=truncation,\n                                  padding=padding,\n                                  return_attention_mask=True,\n                                  max_length=max_length)\n    indexed_tokens = token_output['input_ids']\n    att_masks = token_output['attention_mask']\n    tokens_tensor = torch.tensor(indexed_tokens).to(device)\n    mask_tensor = torch.tensor(att_masks).to(device)\n    text_embed = text_model(tokens_tensor, attention_mask=mask_tensor).last_hidden_state\n    return text_embed","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:51:51.557096Z","iopub.execute_input":"2024-08-07T20:51:51.558032Z","iopub.status.idle":"2024-08-07T20:51:52.876160Z","shell.execute_reply.started":"2024-08-07T20:51:51.557997Z","shell.execute_reply":"2024-08-07T20:51:52.875274Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport glob\nimport os\nimport torch\n\n\ndef load_latents(latent_path):\n    latent_maps = {}\n    for fname in glob.glob(os.path.join(latent_path, '*.pkl')):\n        s = pickle.load(open(fname, 'rb'))\n        for k, v in s.items():\n            latent_maps[k] = v[0]\n    return latent_maps\n\n\ndef drop_text_condition(text_embed, im, empty_text_embed, text_drop_prob):\n    if text_drop_prob > 0:\n        text_drop_mask = torch.zeros((im.shape[0]), device=im.device).float().uniform_(0,\n                                                                                       1) < text_drop_prob\n        assert empty_text_embed is not None, (\"Text Conditioning required as well as\"\n                                        \" text dropping but empty text representation not created\")\n        text_embed[text_drop_mask, :, :] = empty_text_embed[0]\n    return text_embed\n\n\ndef drop_image_condition(image_condition, im, im_drop_prob):\n    if im_drop_prob > 0:\n        im_drop_mask = torch.zeros((im.shape[0], 1, 1, 1), device=im.device).float().uniform_(0,\n                                                                                        1) > im_drop_prob\n        return image_condition * im_drop_mask\n    else:\n        return image_condition\n\n\ndef drop_class_condition(class_condition, class_drop_prob, im):\n    if class_drop_prob > 0:\n        class_drop_mask = torch.zeros((im.shape[0], 1), device=im.device).float().uniform_(0,\n                                                                                           1) > class_drop_prob\n        return class_condition * class_drop_mask\n    else:\n        return class_condition","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:51:57.016999Z","iopub.execute_input":"2024-08-07T20:51:57.017799Z","iopub.status.idle":"2024-08-07T20:51:57.028635Z","shell.execute_reply.started":"2024-08-07T20:51:57.017766Z","shell.execute_reply":"2024-08-07T20:51:57.027658Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import\nfrom collections import namedtuple\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn\nimport torchvision\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef spatial_average(in_tens, keepdim=True):\n    return in_tens.mean([2, 3], keepdim=keepdim)\n\n\nclass vgg16(torch.nn.Module):\n    def __init__(self, requires_grad=False, pretrained=True):\n        super(vgg16, self).__init__()\n        vgg_pretrained_features = torchvision.models.vgg16(pretrained=pretrained).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        self.N_slices = 5\n        for x in range(4):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(4, 9):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(9, 16):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(16, 23):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(23, 30):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu1_2 = h\n        h = self.slice2(h)\n        h_relu2_2 = h\n        h = self.slice3(h)\n        h_relu3_3 = h\n        h = self.slice4(h)\n        h_relu4_3 = h\n        h = self.slice5(h)\n        h_relu5_3 = h\n        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n        return out\n\n\nclass LPIPS(nn.Module):\n    def __init__(self, net='vgg', version='0.1', use_dropout=True):\n        super(LPIPS, self).__init__()\n        self.version = version\n        self.scaling_layer = ScalingLayer()\n\n        self.chns = [64, 128, 256, 512, 512]\n        self.L = len(self.chns)\n        self.net = vgg16(pretrained=True, requires_grad=False)\n\n        self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)\n        self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)\n        self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)\n        self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)\n        self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)\n        self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n        self.lins = nn.ModuleList(self.lins)\n\n        import inspect\n        import os\n        model_path = os.path.abspath(\n            os.path.join(inspect.getfile(self.__init__), '/kaggle/working/', 'weights/v%s/%s.pth' % (version, net)))\n        print('Loading model from: %s' % model_path)\n        self.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n        self.eval()\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def forward(self, in0, in1, normalize=False):\n        if normalize:  # turn on this flag if input is [0,1] so it can be adjusted to [-1, +1]\n            in0 = 2 * in0 - 1\n            in1 = 2 * in1 - 1\n        in0_input, in1_input = self.scaling_layer(in0), self.scaling_layer(in1)\n\n        outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)\n        feats0, feats1, diffs = {}, {}, {}\n        for kk in range(self.L):\n            feats0[kk], feats1[kk] = torch.nn.functional.normalize(outs0[kk], dim=1), torch.nn.functional.normalize(\n                outs1[kk])\n            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n        res = [spatial_average(self.lins[kk](diffs[kk]), keepdim=True) for kk in range(self.L)]\n        val = 0\n\n        for l in range(self.L):\n            val += res[l]\n        return val\n\n\nclass ScalingLayer(nn.Module):\n    def __init__(self):\n        super(ScalingLayer, self).__init__()\n        self.register_buffer('shift', torch.Tensor([-.030, -.088, -.188])[None, :, None, None])\n        self.register_buffer('scale', torch.Tensor([.458, .448, .450])[None, :, None, None])\n\n    def forward(self, inp):\n        return (inp - self.shift) / self.scale\n\n\nclass NetLinLayer(nn.Module):\n    ''' A single linear layer which does a 1x1 conv '''\n\n    def __init__(self, chn_in, chn_out=1, use_dropout=False):\n        super(NetLinLayer, self).__init__()\n\n        layers = [nn.Dropout(), ] if (use_dropout) else []\n        layers += [nn.Conv2d(chn_in, chn_out, 1, stride=1, padding=0, bias=False), ]\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.model(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:57:06.595647Z","iopub.execute_input":"2024-08-07T20:57:06.595968Z","iopub.status.idle":"2024-08-07T20:57:06.625083Z","shell.execute_reply.started":"2024-08-07T20:57:06.595945Z","shell.execute_reply":"2024-08-07T20:57:06.624211Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, im_channels=3,\n                 conv_channels=[64, 128, 256],\n                 kernels=[4,4,4,4],\n                 strides=[2,2,2,1],\n                 paddings=[1,1,1,1]):\n        super().__init__()\n        self.im_channels = im_channels\n        activation = nn.LeakyReLU(0.2)\n        layers_dim = [self.im_channels] + conv_channels + [1]\n        self.layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(layers_dim[i], layers_dim[i + 1],\n                          kernel_size=kernels[i],\n                          stride=strides[i],\n                          padding=paddings[i],\n                          bias=False if i !=0 else True),\n                nn.BatchNorm2d(layers_dim[i + 1]) if i != len(layers_dim) - 2 and i != 0 else nn.Identity(),\n                activation if i != len(layers_dim) - 2 else nn.Identity()\n            )\n            for i in range(len(layers_dim) - 1)\n        ])\n\n    def forward(self, x):\n        out = x\n        for layer in self.layers:\n            out = layer(out)\n        return out\n\n\nif __name__ == '__main__':\n    x = torch.randn((2,3, 256, 256))\n    prob = Discriminator(im_channels=3)(x)\n    print(prob.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:57:16.637281Z","iopub.execute_input":"2024-08-07T20:57:16.637648Z","iopub.status.idle":"2024-08-07T20:57:16.707133Z","shell.execute_reply.started":"2024-08-07T20:57:16.637612Z","shell.execute_reply":"2024-08-07T20:57:16.706177Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"torch.Size([2, 1, 31, 31])\n","output_type":"stream"}]},{"cell_type":"code","source":"import yaml\nimport argparse\nimport torch\nimport random\nimport torchvision\nimport os\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.optim import Adam\nfrom torchvision.utils import make_grid\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ndef train(args):\n    with open(args.config_path, 'r') as file:\n        try:\n            config = yaml.safe_load(file)\n        except yaml.YAMLError as exc:\n            print(exc)\n    print(config)\n\n    dataset_config = config['dataset_params']\n    autoencoder_config = config['autoencoder_params']\n    train_config = config['train_params']\n\n    seed = train_config['seed']\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if device == 'cuda':\n        torch.cuda.manual_seed_all(seed)\n\n    model = VQVAE(im_channels=dataset_config['im_channels'],\n                  model_config=autoencoder_config).to(device)\n    im_dataset_cls = {\n        'celebhq': CelebDataset,\n    }.get(dataset_config['name'])\n\n    im_dataset = im_dataset_cls(split='train',\n                                im_path=dataset_config['im_path'],\n                                im_size=dataset_config['im_size'],\n                                im_channels=dataset_config['im_channels'])\n\n    data_loader = DataLoader(im_dataset,\n                             batch_size=train_config['autoencoder_batch_size'],\n                             shuffle=True)\n\n    if not os.path.exists(train_config['task_name']):\n        os.mkdir(train_config['task_name'])\n\n    num_epochs = train_config['autoencoder_epochs']\n\n    recon_criterion = torch.nn.MSELoss()\n    disc_criterion = torch.nn.MSELoss()\n\n    lpips_model = LPIPS().eval().to(device)\n    discriminator = Discriminator(im_channels=dataset_config['im_channels']).to(device)\n\n    optimizer_d = Adam(discriminator.parameters(), lr=train_config['autoencoder_lr'], betas=(0.5, 0.999))\n    optimizer_g = Adam(model.parameters(), lr=train_config['autoencoder_lr'], betas=(0.5, 0.999))\n\n    disc_step_start = train_config['disc_start']\n    step_count = 0\n    acc_steps = train_config['autoencoder_acc_steps']\n    image_save_steps = train_config['autoencoder_img_save_steps']\n    img_save_count = 0\n\n    for epoch_idx in range(num_epochs):\n        recon_losses = []\n        codebook_losses = []\n        perceptual_losses = []\n        disc_losses = []\n        gen_losses = []\n        losses = []\n\n        optimizer_g.zero_grad()\n        optimizer_d.zero_grad()\n\n        for im in tqdm(data_loader):\n            step_count += 1\n            im = im.float().to(device)\n\n            model_output = model(im)\n            output, z, quantize_losses = model_output\n\n            if step_count % image_save_steps == 0 or step_count == 1:\n                sample_size = min(8, im.shape[0])\n                save_output = torch.clamp(output[:sample_size], -1., 1.).detach().cpu()\n                save_output = ((save_output + 1) / 2)\n                save_input = ((im[:sample_size] + 1) / 2).detach().cpu()\n\n                grid = make_grid(torch.cat([save_input, save_output], dim=0), nrow=sample_size)\n                img = torchvision.transforms.ToPILImage()(grid)\n                if not os.path.exists(os.path.join(train_config['task_name'],'vqvae_autoencoder_samples')):\n                    os.mkdir(os.path.join(train_config['task_name'], 'vqvae_autoencoder_samples'))\n                img.save(os.path.join(train_config['task_name'],'vqvae_autoencoder_samples',\n                                      'current_autoencoder_sample_{}.png'.format(img_save_count)))\n                img_save_count += 1\n                img.close()\n\n            recon_loss = recon_criterion(output, im)\n            recon_losses.append(recon_loss.item())\n            recon_loss = recon_loss / acc_steps\n            g_loss = (recon_loss +\n                      (train_config['codebook_weight'] * quantize_losses['codebook_loss'] / acc_steps) +\n                      (train_config['commitment_beta'] * quantize_losses['commitment_loss'] / acc_steps))\n            codebook_losses.append(train_config['codebook_weight'] * quantize_losses['codebook_loss'].item())\n            # Adversarial loss only if disc_step_start steps passed\n            if step_count > disc_step_start:\n                disc_fake_pred = discriminator(model_output[0])\n                disc_fake_loss = disc_criterion(disc_fake_pred,\n                                                torch.ones(disc_fake_pred.shape,\n                                                           device=disc_fake_pred.device))\n                gen_losses.append(train_config['disc_weight'] * disc_fake_loss.item())\n                g_loss += train_config['disc_weight'] * disc_fake_loss / acc_steps\n            lpips_loss = torch.mean(lpips_model(output, im)) / acc_steps\n            perceptual_losses.append(train_config['perceptual_weight'] * lpips_loss.item())\n            g_loss += train_config['perceptual_weight']*lpips_loss / acc_steps\n            losses.append(g_loss.item())\n            g_loss.backward()\n            if step_count > disc_step_start:\n                fake = output\n                disc_fake_pred = discriminator(fake.detach())\n                disc_real_pred = discriminator(im)\n                disc_fake_loss = disc_criterion(disc_fake_pred,\n                                                torch.zeros(disc_fake_pred.shape,\n                                                            device=disc_fake_pred.device))\n                disc_real_loss = disc_criterion(disc_real_pred,\n                                                torch.ones(disc_real_pred.shape,\n                                                           device=disc_real_pred.device))\n                disc_loss = train_config['disc_weight'] * (disc_fake_loss + disc_real_loss) / 2\n                disc_losses.append(disc_loss.item())\n                disc_loss = disc_loss / acc_steps\n                disc_loss.backward()\n                if step_count % acc_steps == 0:\n                    optimizer_d.step()\n                    optimizer_d.zero_grad()\n\n            if step_count % acc_steps == 0:\n                optimizer_g.step()\n                optimizer_g.zero_grad()\n        optimizer_d.step()\n        optimizer_d.zero_grad()\n        optimizer_g.step()\n        optimizer_g.zero_grad()\n        if len(disc_losses) > 0:\n            print(\n                'Finished epoch: {} | Recon Loss : {:.4f} | Perceptual Loss : {:.4f} | '\n                'Codebook : {:.4f} | G Loss : {:.4f} | D Loss {:.4f}'.\n                format(epoch_idx + 1,\n                       np.mean(recon_losses),\n                       np.mean(perceptual_losses),\n                       np.mean(codebook_losses),\n                       np.mean(gen_losses),\n                       np.mean(disc_losses)))\n        else:\n            print('Finished epoch: {} | Recon Loss : {:.4f} | Perceptual Loss : {:.4f} | Codebook : {:.4f}'.\n                  format(epoch_idx + 1,\n                         np.mean(recon_losses),\n                         np.mean(perceptual_losses),\n                         np.mean(codebook_losses)))\n\n        torch.save(model.state_dict(), os.path.join(train_config['task_name'],\n                                                    train_config['vqvae_autoencoder_ckpt_name']))\n        torch.save(discriminator.state_dict(), os.path.join(train_config['task_name'],\n                                                            train_config['vqvae_discriminator_ckpt_name']))\n    print('Done Training...')\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Arguments for vq vae training')\n    parser.add_argument('--config', dest='config_path',\n                        default='celebhq_text_cond.yaml', type=str)\n\n    args, unknown = parser.parse_known_args()\n    train(args)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:57:23.273542Z","iopub.execute_input":"2024-08-07T20:57:23.273905Z","iopub.status.idle":"2024-08-07T22:53:05.101461Z","shell.execute_reply.started":"2024-08-07T20:57:23.273875Z","shell.execute_reply":"2024-08-07T22:53:05.100593Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"{'dataset_params': {'im_path': 'CelebAMask-HQ', 'im_channels': 3, 'im_size': 256, 'name': 'celebhq'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.00085, 'beta_end': 0.012}, 'ldm_params': {'down_channels': [256, 384, 512, 768], 'mid_channels': [768, 512], 'down_sample': [True, True, True], 'attn_down': [True, True, True], 'time_emb_dim': 512, 'norm_channels': 32, 'num_heads': 16, 'conv_out_channels': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2, 'condition_config': {'condition_types': ['text'], 'text_condition_config': {'text_embed_model': 'clip', 'text_embed_dim': 512, 'cond_drop_prob': 0.1}}}, 'autoencoder_params': {'z_channels': 3, 'codebook_size': 8192, 'down_channels': [64, 128, 256, 256], 'mid_channels': [256, 256], 'down_sample': [True, True, True], 'attn_down': [False, False, False], 'norm_channels': 32, 'num_heads': 4, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2}, 'train_params': {'seed': 1111, 'task_name': 'celebhq', 'ldm_batch_size': 16, 'autoencoder_batch_size': 8, 'disc_start': 15000, 'disc_weight': 0.5, 'codebook_weight': 1, 'commitment_beta': 0.2, 'perceptual_weight': 1, 'kl_weight': 5e-06, 'ldm_epochs': 1, 'autoencoder_epochs': 1, 'num_samples': 1, 'num_grid_rows': 1, 'ldm_lr': 5e-06, 'autoencoder_lr': 1e-05, 'autoencoder_acc_steps': 4, 'autoencoder_img_save_steps': 64, 'save_latents': False, 'cf_guidance_scale': 1.0, 'vae_latent_dir_name': 'vae_latents', 'vqvae_latent_dir_name': 'vqvae_latents', 'ldm_ckpt_name': 'ddpm_ckpt_text_cond_clip.pth', 'vqvae_autoencoder_ckpt_name': 'vqvae_autoencoder_ckpt.pth', 'vae_autoencoder_ckpt_name': 'vae_autoencoder_ckpt.pth', 'vqvae_discriminator_ckpt_name': 'vqvae_discriminator_ckpt.pth', 'vae_discriminator_ckpt_name': 'vae_discriminator_ckpt.pth'}}\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 30000/30000 [00:00<00:00, 1376234.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Found 30000 images\nFound 0 masks\nFound 0 captions\nLoading model from: /kaggle/working/weights/v0.1/vgg.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3750/3750 [1:55:38<00:00,  1.85s/it]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch: 1 | Recon Loss : 0.0562 | Perceptual Loss : 0.1307 | Codebook : 0.0041\nDone Training...\n","output_type":"stream"}]},{"cell_type":"code","source":"import argparse\nimport glob\nimport os\nimport pickle\n\nimport torch\nimport torchvision\nimport yaml\nfrom torch.utils.data.dataloader import DataLoader\nfrom torchvision.utils import make_grid\nfrom tqdm import tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ndef infer(args):\n    ######## Read the config file #######\n    with open(args.config_path, 'r') as file:\n        try:\n            config = yaml.safe_load(file)\n        except yaml.YAMLError as exc:\n            print(exc)\n    print(config)\n\n    dataset_config = config['dataset_params']\n    autoencoder_config = config['autoencoder_params']\n    train_config = config['train_params']\n\n    # Create the dataset\n    im_dataset_cls = {\n        'celebhq': CelebDataset\n    }.get(dataset_config['name'])\n\n    im_dataset = im_dataset_cls(split='train',\n                                im_path=dataset_config['im_path'],\n                                im_size=dataset_config['im_size'],\n                                im_channels=dataset_config['im_channels'])\n\n    # This is only used for saving latents. Which as of now\n    # is not done in batches hence batch size 1\n    data_loader = DataLoader(im_dataset,\n                             batch_size=1,\n                             shuffle=False)\n\n    num_images = train_config['num_samples']\n    ngrid = train_config['num_grid_rows']\n\n    idxs = torch.randint(0, len(im_dataset) - 1, (num_images,))\n    ims = torch.cat([im_dataset[idx][None, :] for idx in idxs]).float()\n    ims = ims.to(device)\n\n    model = VQVAE(im_channels=dataset_config['im_channels'],\n                  model_config=autoencoder_config).to(device)\n    model.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n                                                    train_config['vqvae_autoencoder_ckpt_name']),\n                                     map_location=device))\n    model.eval()\n\n    with torch.no_grad():\n\n        encoded_output, _ = model.encode(ims)\n        decoded_output = model.decode(encoded_output)\n        encoded_output = torch.clamp(encoded_output, -1., 1.)\n        encoded_output = (encoded_output + 1) / 2\n        decoded_output = torch.clamp(decoded_output, -1., 1.)\n        decoded_output = (decoded_output + 1) / 2\n        ims = (ims + 1) / 2\n\n        encoder_grid = make_grid(encoded_output.cpu(), nrow=ngrid)\n        decoder_grid = make_grid(decoded_output.cpu(), nrow=ngrid)\n        input_grid = make_grid(ims.cpu(), nrow=ngrid)\n        encoder_grid = torchvision.transforms.ToPILImage()(encoder_grid)\n        decoder_grid = torchvision.transforms.ToPILImage()(decoder_grid)\n        input_grid = torchvision.transforms.ToPILImage()(input_grid)\n\n        input_grid.save(os.path.join(train_config['task_name'], 'input_samples.png'))\n        encoder_grid.save(os.path.join(train_config['task_name'], 'encoded_samples.png'))\n        decoder_grid.save(os.path.join(train_config['task_name'], 'reconstructed_samples.png'))\n\n        if train_config['save_latents']:\n            # save Latents (but in a very unoptimized way)\n            latent_path = os.path.join(train_config['task_name'], train_config['vqvae_latent_dir_name'])\n            latent_fnames = glob.glob(os.path.join(train_config['task_name'], train_config['vqvae_latent_dir_name'],\n                                                   '*.pkl'))\n            assert len(latent_fnames) == 0, 'Latents already present. Delete all latent files and re-run'\n            if not os.path.exists(latent_path):\n                os.mkdir(latent_path)\n            print('Saving Latents for {}'.format(dataset_config['name']))\n\n            fname_latent_map = {}\n            part_count = 0\n            count = 0\n            for idx, im in enumerate(tqdm(data_loader)):\n                encoded_output, _ = model.encode(im.float().to(device))\n                fname_latent_map[im_dataset.images[idx]] = encoded_output.cpu()\n                # Save latents every 1000 images\n                if (count+1) % 1000 == 0:\n                    pickle.dump(fname_latent_map, open(os.path.join(latent_path,\n                                                                    '{}.pkl'.format(part_count)), 'wb'))\n                    part_count += 1\n                    fname_latent_map = {}\n                count += 1\n            if len(fname_latent_map) > 0:\n                pickle.dump(fname_latent_map, open(os.path.join(latent_path,\n                                                   '{}.pkl'.format(part_count)), 'wb'))\n            print('Done saving latents')\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Arguments for ddpm training')\n    parser.add_argument('--config', dest='config_path',\n                        default='celebhq_text_cond.yaml', type=str)\n\n    args, unknown = parser.parse_known_args()\n    train(args)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T22:53:19.593058Z","iopub.execute_input":"2024-08-07T22:53:19.593383Z","iopub.status.idle":"2024-08-08T00:48:59.663835Z","shell.execute_reply.started":"2024-08-07T22:53:19.593358Z","shell.execute_reply":"2024-08-08T00:48:59.662897Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"{'dataset_params': {'im_path': 'CelebAMask-HQ', 'im_channels': 3, 'im_size': 256, 'name': 'celebhq'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.00085, 'beta_end': 0.012}, 'ldm_params': {'down_channels': [256, 384, 512, 768], 'mid_channels': [768, 512], 'down_sample': [True, True, True], 'attn_down': [True, True, True], 'time_emb_dim': 512, 'norm_channels': 32, 'num_heads': 16, 'conv_out_channels': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2, 'condition_config': {'condition_types': ['text'], 'text_condition_config': {'text_embed_model': 'clip', 'text_embed_dim': 512, 'cond_drop_prob': 0.1}}}, 'autoencoder_params': {'z_channels': 3, 'codebook_size': 8192, 'down_channels': [64, 128, 256, 256], 'mid_channels': [256, 256], 'down_sample': [True, True, True], 'attn_down': [False, False, False], 'norm_channels': 32, 'num_heads': 4, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2}, 'train_params': {'seed': 1111, 'task_name': 'celebhq', 'ldm_batch_size': 16, 'autoencoder_batch_size': 8, 'disc_start': 15000, 'disc_weight': 0.5, 'codebook_weight': 1, 'commitment_beta': 0.2, 'perceptual_weight': 1, 'kl_weight': 5e-06, 'ldm_epochs': 1, 'autoencoder_epochs': 1, 'num_samples': 1, 'num_grid_rows': 1, 'ldm_lr': 5e-06, 'autoencoder_lr': 1e-05, 'autoencoder_acc_steps': 4, 'autoencoder_img_save_steps': 64, 'save_latents': False, 'cf_guidance_scale': 1.0, 'vae_latent_dir_name': 'vae_latents', 'vqvae_latent_dir_name': 'vqvae_latents', 'ldm_ckpt_name': 'ddpm_ckpt_text_cond_clip.pth', 'vqvae_autoencoder_ckpt_name': 'vqvae_autoencoder_ckpt.pth', 'vae_autoencoder_ckpt_name': 'vae_autoencoder_ckpt.pth', 'vqvae_discriminator_ckpt_name': 'vqvae_discriminator_ckpt.pth', 'vae_discriminator_ckpt_name': 'vae_discriminator_ckpt.pth'}}\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 30000/30000 [00:00<00:00, 1308620.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Found 30000 images\nFound 0 masks\nFound 0 captions\nLoading model from: /kaggle/working/weights/v0.1/vgg.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3750/3750 [1:55:36<00:00,  1.85s/it]  \n","output_type":"stream"},{"name":"stdout","text":"Finished epoch: 1 | Recon Loss : 0.0551 | Perceptual Loss : 0.1327 | Codebook : 0.0040\nDone Training...\n","output_type":"stream"}]},{"cell_type":"code","source":"import yaml\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ndef train(args):\n    with open(args.config_path, 'r') as file:\n        try:\n            config = yaml.safe_load(file)\n        except yaml.YAMLError as exc:\n            print(exc)\n    print(config)\n\n    diffusion_config = config['diffusion_params']\n    dataset_config = config['dataset_params']\n    diffusion_model_config = config['ldm_params']\n    autoencoder_model_config = config['autoencoder_params']\n    train_config = config['train_params']\n\n    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n                                     beta_start=diffusion_config['beta_start'],\n                                     beta_end=diffusion_config['beta_end'])\n\n    text_tokenizer = None\n    text_model = None\n    empty_text_embed = None\n    condition_types = []\n    condition_config = get_config_value(diffusion_model_config, key='condition_config', default_value=None)\n    if condition_config is not None:\n        assert 'condition_types' in condition_config, \\\n            \"condition type missing in conditioning config\"\n        condition_types = condition_config['condition_types']\n        if 'text' in condition_types:\n            validate_text_config(condition_config)\n            with torch.no_grad():\n                # Load tokenizer and text model based on config\n                # Also get empty text representation\n                text_tokenizer, text_model = get_tokenizer_and_model(condition_config['text_condition_config']\n                                                                     ['text_embed_model'], device=device)\n                empty_text_embed = get_text_representation([''], text_tokenizer, text_model, device)\n\n    im_dataset_cls = {\n        'celebhq': CelebDataset,\n    }.get(dataset_config['name'])\n\n    im_dataset = im_dataset_cls(split='train',\n                                im_path=dataset_config['im_path'],\n                                im_size=dataset_config['im_size'],\n                                im_channels=dataset_config['im_channels'],\n                                use_latents=True,\n                                latent_path=os.path.join(train_config['task_name'],\n                                                         train_config['vqvae_latent_dir_name']),\n                                condition_config=condition_config)\n\n    data_loader = DataLoader(im_dataset,\n                             batch_size=train_config['ldm_batch_size'],\n                             shuffle=True)\n\n    model = Unet(im_channels=autoencoder_model_config['z_channels'],\n                 model_config=diffusion_model_config).to(device)\n    model.train()\n\n    vae = None\n    if not im_dataset.use_latents:\n        print('Loading vqvae model as latents not present')\n        vae = VQVAE(im_channels=dataset_config['im_channels'],\n                    model_config=autoencoder_model_config).to(device)\n        vae.eval()\n        if os.path.exists(os.path.join(train_config['task_name'],\n                                       train_config['vqvae_autoencoder_ckpt_name'])):\n            print('Loaded vae checkpoint')\n            vae.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n                                                        train_config['vqvae_autoencoder_ckpt_name']),\n                                           map_location=device))\n        else:\n            raise Exception('VAE checkpoint not found and use_latents was disabled')\n\n    num_epochs = train_config['ldm_epochs']\n    optimizer = Adam(model.parameters(), lr=train_config['ldm_lr'])\n    criterion = torch.nn.MSELoss()\n\n    if not im_dataset.use_latents:\n        assert vae is not None\n        for param in vae.parameters():\n            param.requires_grad = False\n\n    for epoch_idx in range(num_epochs):\n        losses = []\n        for data in tqdm(data_loader):\n            cond_input = None\n            if condition_config is not None:\n                im, cond_input = data\n            else:\n                im = data\n            optimizer.zero_grad()\n            im = im.float().to(device)\n            if not im_dataset.use_latents:\n                with torch.no_grad():\n                    im, _ = vae.encode(im)\n\n            if 'text' in condition_types:\n                with torch.no_grad():\n                    assert 'text' in cond_input, 'Conditioning Type Text but no text conditioning input present'\n                    validate_text_config(condition_config)\n                    text_condition = get_text_representation(cond_input['text'],\n                                                                 text_tokenizer,\n                                                                 text_model,\n                                                                 device)\n                    text_drop_prob = get_config_value(condition_config['text_condition_config'],\n                                                      'cond_drop_prob', 0.)\n                    text_condition = drop_text_condition(text_condition, im, empty_text_embed, text_drop_prob)\n                    cond_input['text'] = text_condition\n            if 'image' in condition_types:\n                assert 'image' in cond_input, 'Conditioning Type Image but no image conditioning input present'\n                validate_image_config(condition_config)\n                cond_input_image = cond_input['image'].to(device)\n                im_drop_prob = get_config_value(condition_config['image_condition_config'],\n                                                      'cond_drop_prob', 0.)\n                cond_input['image'] = drop_image_condition(cond_input_image, im, im_drop_prob)\n            if 'class' in condition_types:\n                assert 'class' in cond_input, 'Conditioning Type Class but no class conditioning input present'\n                validate_class_config(condition_config)\n                class_condition = torch.nn.functional.one_hot(\n                    cond_input['class'],\n                    condition_config['class_condition_config']['num_classes']).to(device)\n                class_drop_prob = get_config_value(condition_config['class_condition_config'],\n                                                   'cond_drop_prob', 0.)\n                cond_input['class'] = drop_class_condition(class_condition, class_drop_prob, im)\n\n            noise = torch.randn_like(im).to(device)\n\n            t = torch.randint(0, diffusion_config['num_timesteps'], (im.shape[0],)).to(device)\n\n            noisy_im = scheduler.add_noise(im, noise, t)\n            noise_pred = model(noisy_im, t, cond_input=cond_input)\n            loss = criterion(noise_pred, noise)\n            losses.append(loss.item())\n            loss.backward()\n            optimizer.step()\n        print('Finished epoch:{} | Loss : {:.4f}'.format(\n            epoch_idx + 1,\n            np.mean(losses)))\n        torch.save(model.state_dict(), os.path.join(train_config['task_name'],\n                                                    train_config['ldm_ckpt_name']))\n\n    print('Done Training ...')\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Arguments for ddpm training')\n    parser.add_argument('--config', dest='config_path',\n                        default='celebhq_text_cond.yaml', type=str)\n\n    args, unknown = parser.parse_known_args()\n    train(args)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T00:49:51.452467Z","iopub.execute_input":"2024-08-08T00:49:51.452844Z","iopub.status.idle":"2024-08-08T01:34:19.178283Z","shell.execute_reply.started":"2024-08-08T00:49:51.452813Z","shell.execute_reply":"2024-08-08T01:34:19.177367Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"{'dataset_params': {'im_path': 'CelebAMask-HQ', 'im_channels': 3, 'im_size': 256, 'name': 'celebhq'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.00085, 'beta_end': 0.012}, 'ldm_params': {'down_channels': [256, 384, 512, 768], 'mid_channels': [768, 512], 'down_sample': [True, True, True], 'attn_down': [True, True, True], 'time_emb_dim': 512, 'norm_channels': 32, 'num_heads': 16, 'conv_out_channels': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2, 'condition_config': {'condition_types': ['text'], 'text_condition_config': {'text_embed_model': 'clip', 'text_embed_dim': 512, 'cond_drop_prob': 0.1}}}, 'autoencoder_params': {'z_channels': 3, 'codebook_size': 8192, 'down_channels': [64, 128, 256, 256], 'mid_channels': [256, 256], 'down_sample': [True, True, True], 'attn_down': [False, False, False], 'norm_channels': 32, 'num_heads': 4, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2}, 'train_params': {'seed': 1111, 'task_name': 'celebhq', 'ldm_batch_size': 16, 'autoencoder_batch_size': 8, 'disc_start': 15000, 'disc_weight': 0.5, 'codebook_weight': 1, 'commitment_beta': 0.2, 'perceptual_weight': 1, 'kl_weight': 5e-06, 'ldm_epochs': 1, 'autoencoder_epochs': 1, 'num_samples': 1, 'num_grid_rows': 1, 'ldm_lr': 5e-06, 'autoencoder_lr': 1e-05, 'autoencoder_acc_steps': 4, 'autoencoder_img_save_steps': 64, 'save_latents': False, 'cf_guidance_scale': 1.0, 'vae_latent_dir_name': 'vae_latents', 'vqvae_latent_dir_name': 'vqvae_latents', 'ldm_ckpt_name': 'ddpm_ckpt_text_cond_clip.pth', 'vqvae_autoencoder_ckpt_name': 'vqvae_autoencoder_ckpt.pth', 'vae_autoencoder_ckpt_name': 'vae_autoencoder_ckpt.pth', 'vqvae_discriminator_ckpt_name': 'vqvae_discriminator_ckpt.pth', 'vae_discriminator_ckpt_name': 'vae_discriminator_ckpt.pth'}}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad19ad6cfd944624aee537612e53a6c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf6a1295b0ef41b09acb0ed9b752e67a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4225e70914847f1b182da6f278f12a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0afe87bafb534e53b4cf41c602e0d636"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb60354fba214479b824a999dc8e5869"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"705995f4b9384f20b1c6d6ee64e76b9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3d2515b04e04b3598311ba233c43fb3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n100%|██████████| 30000/30000 [00:01<00:00, 15916.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Found 30000 images\nFound 0 masks\nFound 30000 captions\nLatents not found\nLoading vqvae model as latents not present\nLoaded vae checkpoint\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1875/1875 [44:17<00:00,  1.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"Finished epoch:1 | Loss : 0.1773\nDone Training ...\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}