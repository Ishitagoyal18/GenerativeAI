{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyObUV88U7PU+bWv68FgERN7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QHdH874GjSXD"},"outputs":[],"source":["!unzip data"]},{"cell_type":"code","source":["! gdown 1badu11NqxGf6qM3PTTooQDJvQbejgbTv"],"metadata":{"id":"wHyUex2bnYJz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! unzip CelebAMask-HQ.zip"],"metadata":{"id":"eH3OSvlVrXyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import absolute_import\n","from collections import namedtuple\n","import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","from torch.autograd import Variable\n","import numpy as np\n","import torchvision\n"],"metadata":{"id":"zNbqNxvvwsBW","executionInfo":{"status":"ok","timestamp":1720503400474,"user_tz":-330,"elapsed":6014,"user":{"displayName":"Ishita","userId":"07336242852543837225"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#LPIPS\n","\n","# Taken from https://github.com/richzhang/PerceptualSimilarity/blob/master/lpips/lpips.py\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","def spatial_average(in_tens, keepdim=True):\n","    return in_tens.mean([2, 3], keepdim=keepdim)\n","\n","\n","class vgg16(torch.nn.Module):\n","    def __init__(self, requires_grad=False, pretrained=True):\n","        super(vgg16, self).__init__()\n","        # Load pretrained vgg model from torchvision\n","        vgg_pretrained_features = torchvision.models.vgg16(pretrained=pretrained).features\n","        self.slice1 = nn.Sequential()\n","        self.slice2 = nn.Sequential()\n","        self.slice3 = nn.Sequential()\n","        self.slice4 = nn.Sequential()\n","        self.slice5 = nn.Sequential()\n","        self.N_slices = 5\n","        for x in range(4):\n","            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n","        for x in range(4, 9):\n","            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n","        for x in range(9, 16):\n","            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n","        for x in range(16, 23):\n","            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n","        for x in range(23, 30):\n","            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n","\n","\n","        if not requires_grad:\n","            for param in self.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, X):\n","\n","        h = self.slice1(X)\n","        h_relu1_2 = h\n","        h = self.slice2(h)\n","        h_relu2_2 = h\n","        h = self.slice3(h)\n","        h_relu3_3 = h\n","        h = self.slice4(h)\n","        h_relu4_3 = h\n","        h = self.slice5(h)\n","        h_relu5_3 = h\n","        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n","        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n","        return out\n","\n","\n","# Learned perceptual metric\n","class LPIPS(nn.Module):\n","    def __init__(self, net='vgg', version='0.1', use_dropout=True):\n","        super(LPIPS, self).__init__()\n","        self.version = version\n","        self.scaling_layer = ScalingLayer()\n","\n","\n","        self.chns = [64, 128, 256, 512, 512]\n","        self.L = len(self.chns)\n","        self.net = vgg16(pretrained=True, requires_grad=False)\n","\n","\n","        self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)\n","        self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)\n","        self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)\n","        self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)\n","        self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)\n","        self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n","        self.lins = nn.ModuleList(self.lins)\n","\n","\n","\n","        import inspect\n","        import os\n","\n","        model_path = os.path.abspath('%s.pth' % (net))\n","\n","        print('Loading model from: %s' % model_path)\n","        self.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n","\n","\n","        self.eval()\n","        for param in self.parameters():\n","            param.requires_grad = False\n","\n","\n","    def forward(self, in0, in1, normalize=False):\n","\n","        if normalize:\n","            in0 = 2 * in0 - 1\n","            in1 = 2 * in1 - 1\n","\n","        in0_input, in1_input = self.scaling_layer(in0), self.scaling_layer(in1)\n","\n","        outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)\n","        feats0, feats1, diffs = {}, {}, {}\n","\n","        for kk in range(self.L):\n","            feats0[kk], feats1[kk] = nn.functional.normalize(outs0[kk], dim=1), nn.functional.normalize(\n","                outs1[kk])\n","            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n","\n","        res = [spatial_average(self.lins[kk](diffs[kk]), keepdim=True) for kk in range(self.L)]\n","        val = 0\n","\n","        # Aggregate the results of each layer\n","        for l in range(self.L):\n","            val += res[l]\n","        return val\n","\n","\n","class ScalingLayer(nn.Module):\n","    def __init__(self):\n","        super(ScalingLayer, self).__init__()\n","        # Imagnet normalization for (0-1)\n","        # mean = [0.485, 0.456, 0.406]\n","        # std = [0.229, 0.224, 0.225]\n","        self.register_buffer('shift', torch.Tensor([-.030, -.088, -.188])[None, :, None, None])\n","        self.register_buffer('scale', torch.Tensor([.458, .448, .450])[None, :, None, None])\n","\n","    def forward(self, inp):\n","        return (inp - self.shift) / self.scale\n","\n","\n","class NetLinLayer(nn.Module):\n","    ''' A single linear layer which does a 1x1 conv '''\n","\n","    def __init__(self, chn_in, chn_out=1, use_dropout=False):\n","        super(NetLinLayer, self).__init__()\n","\n","        layers = [nn.Dropout(), ] if (use_dropout) else []\n","        layers += [nn.Conv2d(chn_in, chn_out, 1, stride=1, padding=0, bias=False), ]\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = self.model(x)\n","        return out"],"metadata":{"id":"-fV-lz46utG_","executionInfo":{"status":"ok","timestamp":1720503556948,"user_tz":-330,"elapsed":1118,"user":{"displayName":"Ishita","userId":"07336242852543837225"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["#Blocks\n","\n","def get_time_embedding(time_steps, temb_dim):\n","\n","    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n","\n","    # factor = 10000^(2i/d_model)\n","    factor = 10000 ** ((torch.arange(\n","        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n","    )\n","\n","    # pos / factor\n","    # timesteps B -> B, 1 -> B, temb_dim\n","    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n","    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n","    return t_emb\n","\n","\n","class DownBlock(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, t_emb_dim,\n","                 down_sample, num_heads, num_layers, attn, norm_channels, cross_attn=False, context_dim=None):\n","        super().__init__()\n","        self.num_layers = num_layers\n","        self.down_sample = down_sample\n","        self.attn = attn\n","        self.context_dim = context_dim\n","        self.cross_attn = cross_attn\n","        self.t_emb_dim = t_emb_dim\n","        self.resnet_conv_first = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n","                    nn.SiLU(),\n","                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n","                              kernel_size=3, stride=1, padding=1),\n","                )\n","                for i in range(num_layers)\n","            ]\n","        )\n","        if self.t_emb_dim is not None:\n","            self.t_emb_layers = nn.ModuleList([\n","                nn.Sequential(\n","                    nn.SiLU(),\n","                    nn.Linear(self.t_emb_dim, out_channels)\n","                )\n","                for _ in range(num_layers)\n","            ])\n","        self.resnet_conv_second = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.GroupNorm(norm_channels, out_channels),\n","                    nn.SiLU(),\n","                    nn.Conv2d(out_channels, out_channels,\n","                              kernel_size=3, stride=1, padding=1),\n","                )\n","                for _ in range(num_layers)\n","            ]\n","        )\n","\n","        if self.attn:\n","            self.attention_norms = nn.ModuleList(\n","                [nn.GroupNorm(norm_channels, out_channels)\n","                 for _ in range(num_layers)]\n","            )\n","\n","            self.attentions = nn.ModuleList(\n","                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n","                 for _ in range(num_layers)]\n","            )\n","\n","        if self.cross_attn:\n","            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n","            self.cross_attention_norms = nn.ModuleList(\n","                [nn.GroupNorm(norm_channels, out_channels)\n","                 for _ in range(num_layers)]\n","            )\n","            self.cross_attentions = nn.ModuleList(\n","                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n","                 for _ in range(num_layers)]\n","            )\n","            self.context_proj = nn.ModuleList(\n","                [nn.Linear(context_dim, out_channels)\n","                 for _ in range(num_layers)]\n","            )\n","\n","        self.residual_input_conv = nn.ModuleList(\n","            [\n","                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n","                for i in range(num_layers)\n","            ]\n","        )\n","        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n","                                          4, 2, 1) if self.down_sample else nn.Identity()\n","\n","    def forward(self, x, t_emb=None, context=None):\n","        out = x\n","        for i in range(self.num_layers):\n","            # Resnet block of Unet\n","            resnet_input = out\n","            out = self.resnet_conv_first[i](out)\n","            if self.t_emb_dim is not None:\n","                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n","            out = self.resnet_conv_second[i](out)\n","            out = out + self.residual_input_conv[i](resnet_input)\n","\n","            if self.attn:\n","                # Attention block of Unet\n","                batch_size, channels, h, w = out.shape\n","                in_attn = out.reshape(batch_size, channels, h * w)\n","                in_attn = self.attention_norms[i](in_attn)\n","                in_attn = in_attn.transpose(1, 2)\n","                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n","                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n","                out = out + out_attn\n","\n","            if self.cross_attn:\n","                assert context is not None, \"context cannot be None if cross attention layers are used\"\n","                batch_size, channels, h, w = out.shape\n","                in_attn = out.reshape(batch_size, channels, h * w)\n","                in_attn = self.cross_attention_norms[i](in_attn)\n","                in_attn = in_attn.transpose(1, 2)\n","                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n","                context_proj = self.context_proj[i](context)\n","                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n","                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n","                out = out + out_attn\n","\n","        # Downsample\n","        out = self.down_sample_conv(out)\n","        return out\n","\n","\n","class MidBlock(nn.Module):\n","\n","\n","    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads, num_layers, norm_channels, cross_attn=None, context_dim=None):\n","        super().__init__()\n","        self.num_layers = num_layers\n","        self.t_emb_dim = t_emb_dim\n","        self.context_dim = context_dim\n","        self.cross_attn = cross_attn\n","        self.resnet_conv_first = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n","                    nn.SiLU(),\n","                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n","                              padding=1),\n","                )\n","                for i in range(num_layers + 1)\n","            ]\n","        )\n","\n","        if self.t_emb_dim is not None:\n","            self.t_emb_layers = nn.ModuleList([\n","                nn.Sequential(\n","                    nn.SiLU(),\n","                    nn.Linear(t_emb_dim, out_channels)\n","                )\n","                for _ in range(num_layers + 1)\n","            ])\n","        self.resnet_conv_second = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.GroupNorm(norm_channels, out_channels),\n","                    nn.SiLU(),\n","                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","                )\n","                for _ in range(num_layers + 1)\n","            ]\n","        )\n","\n","        self.attention_norms = nn.ModuleList(\n","            [nn.GroupNorm(norm_channels, out_channels)\n","             for _ in range(num_layers)]\n","        )\n","\n","        self.attentions = nn.ModuleList(\n","            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n","             for _ in range(num_layers)]\n","        )\n","        if self.cross_attn:\n","            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n","            self.cross_attention_norms = nn.ModuleList(\n","                [nn.GroupNorm(norm_channels, out_channels)\n","                 for _ in range(num_layers)]\n","            )\n","            self.cross_attentions = nn.ModuleList(\n","                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n","                 for _ in range(num_layers)]\n","            )\n","            self.context_proj = nn.ModuleList(\n","                [nn.Linear(context_dim, out_channels)\n","                 for _ in range(num_layers)]\n","            )\n","        self.residual_input_conv = nn.ModuleList(\n","            [\n","                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n","                for i in range(num_layers + 1)\n","            ]\n","        )\n","\n","    def forward(self, x, t_emb=None, context=None):\n","        out = x\n","\n","        # First resnet block\n","        resnet_input = out\n","        out = self.resnet_conv_first[0](out)\n","        if self.t_emb_dim is not None:\n","            out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n","        out = self.resnet_conv_second[0](out)\n","        out = out + self.residual_input_conv[0](resnet_input)\n","\n","        for i in range(self.num_layers):\n","            # Attention Block\n","            batch_size, channels, h, w = out.shape\n","            in_attn = out.reshape(batch_size, channels, h * w)\n","            in_attn = self.attention_norms[i](in_attn)\n","            in_attn = in_attn.transpose(1, 2)\n","            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n","            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n","            out = out + out_attn\n","\n","            if self.cross_attn:\n","                assert context is not None, \"context cannot be None if cross attention layers are used\"\n","                batch_size, channels, h, w = out.shape\n","                in_attn = out.reshape(batch_size, channels, h * w)\n","                in_attn = self.cross_attention_norms[i](in_attn)\n","                in_attn = in_attn.transpose(1, 2)\n","                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n","                context_proj = self.context_proj[i](context)\n","                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n","                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n","                out = out + out_attn\n","\n","\n","            # Resnet Block\n","            resnet_input = out\n","            out = self.resnet_conv_first[i + 1](out)\n","            if self.t_emb_dim is not None:\n","                out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n","            out = self.resnet_conv_second[i + 1](out)\n","            out = out + self.residual_input_conv[i + 1](resnet_input)\n","\n","        return out\n","\n","\n","class UpBlock(nn.Module):\n","\n","\n","    def __init__(self, in_channels, out_channels, t_emb_dim,\n","                 up_sample, num_heads, num_layers, attn, norm_channels):\n","        super().__init__()\n","        self.num_layers = num_layers\n","        self.up_sample = up_sample\n","        self.t_emb_dim = t_emb_dim\n","        self.attn = attn\n","        self.resnet_conv_first = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n","                    nn.SiLU(),\n","                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n","                              padding=1),\n","                )\n","                for i in range(num_layers)\n","            ]\n","        )\n","\n","        if self.t_emb_dim is not None:\n","            self.t_emb_layers = nn.ModuleList([\n","                nn.Sequential(\n","                    nn.SiLU(),\n","                    nn.Linear(t_emb_dim, out_channels)\n","                )\n","                for _ in range(num_layers)\n","            ])\n","\n","        self.resnet_conv_second = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.GroupNorm(norm_channels, out_channels),\n","                    nn.SiLU(),\n","                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","                )\n","                for _ in range(num_layers)\n","            ]\n","        )\n","        if self.attn:\n","            self.attention_norms = nn.ModuleList(\n","                [\n","                    nn.GroupNorm(norm_channels, out_channels)\n","                    for _ in range(num_layers)\n","                ]\n","            )\n","\n","            self.attentions = nn.ModuleList(\n","                [\n","                    nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n","                    for _ in range(num_layers)\n","                ]\n","            )\n","\n","        self.residual_input_conv = nn.ModuleList(\n","            [\n","                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n","                for i in range(num_layers)\n","            ]\n","        )\n","        self.up_sample_conv = nn.ConvTranspose2d(in_channels, in_channels,\n","                                                 4, 2, 1) \\\n","            if self.up_sample else nn.Identity()\n","\n","    def forward(self, x, out_down=None, t_emb=None):\n","        # Upsample\n","        x = self.up_sample_conv(x)\n","\n","        # Concat with Downblock output\n","        if out_down is not None:\n","            x = torch.cat([x, out_down], dim=1)\n","\n","        out = x\n","        for i in range(self.num_layers):\n","            # Resnet Block\n","            resnet_input = out\n","            out = self.resnet_conv_first[i](out)\n","            if self.t_emb_dim is not None:\n","                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n","            out = self.resnet_conv_second[i](out)\n","            out = out + self.residual_input_conv[i](resnet_input)\n","\n","            # Self Attention\n","            if self.attn:\n","                batch_size, channels, h, w = out.shape\n","                in_attn = out.reshape(batch_size, channels, h * w)\n","                in_attn = self.attention_norms[i](in_attn)\n","                in_attn = in_attn.transpose(1, 2)\n","                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n","                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n","                out = out + out_attn\n","        return out\n","\n","\n","class UpBlockUnet(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample,\n","                 num_heads, num_layers, norm_channels, cross_attn=False, context_dim=None):\n","        super().__init__()\n","        self.num_layers = num_layers\n","        self.up_sample = up_sample\n","        self.t_emb_dim = t_emb_dim\n","        self.cross_attn = cross_attn\n","        self.context_dim = context_dim\n","        self.resnet_conv_first = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n","                    nn.SiLU(),\n","                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n","                              padding=1),\n","                )\n","                for i in range(num_layers)\n","            ]\n","        )\n","\n","        if self.t_emb_dim is not None:\n","            self.t_emb_layers = nn.ModuleList([\n","                nn.Sequential(\n","                    nn.SiLU(),\n","                    nn.Linear(t_emb_dim, out_channels)\n","                )\n","                for _ in range(num_layers)\n","            ])\n","\n","        self.resnet_conv_second = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.GroupNorm(norm_channels, out_channels),\n","                    nn.SiLU(),\n","                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","                )\n","                for _ in range(num_layers)\n","            ]\n","        )\n","\n","        self.attention_norms = nn.ModuleList(\n","            [\n","                nn.GroupNorm(norm_channels, out_channels)\n","                for _ in range(num_layers)\n","            ]\n","        )\n","\n","        self.attentions = nn.ModuleList(\n","            [\n","                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n","                for _ in range(num_layers)\n","            ]\n","        )\n","\n","        if self.cross_attn:\n","            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n","            self.cross_attention_norms = nn.ModuleList(\n","                [nn.GroupNorm(norm_channels, out_channels)\n","                 for _ in range(num_layers)]\n","            )\n","            self.cross_attentions = nn.ModuleList(\n","                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n","                 for _ in range(num_layers)]\n","            )\n","            self.context_proj = nn.ModuleList(\n","                [nn.Linear(context_dim, out_channels)\n","                 for _ in range(num_layers)]\n","            )\n","        self.residual_input_conv = nn.ModuleList(\n","            [\n","                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n","                for i in range(num_layers)\n","            ]\n","        )\n","        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n","                                                 4, 2, 1) \\\n","            if self.up_sample else nn.Identity()\n","\n","    def forward(self, x, out_down=None, t_emb=None, context=None):\n","        x = self.up_sample_conv(x)\n","        if out_down is not None:\n","            x = torch.cat([x, out_down], dim=1)\n","\n","        out = x\n","        for i in range(self.num_layers):\n","            # Resnet\n","            resnet_input = out\n","            out = self.resnet_conv_first[i](out)\n","            if self.t_emb_dim is not None:\n","                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n","            out = self.resnet_conv_second[i](out)\n","            out = out + self.residual_input_conv[i](resnet_input)\n","            # Self Attention\n","            batch_size, channels, h, w = out.shape\n","            in_attn = out.reshape(batch_size, channels, h * w)\n","            in_attn = self.attention_norms[i](in_attn)\n","            in_attn = in_attn.transpose(1, 2)\n","            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n","            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n","            out = out + out_attn\n","            # Cross Attention\n","            if self.cross_attn:\n","                assert context is not None, \"context cannot be None if cross attention layers are used\"\n","                batch_size, channels, h, w = out.shape\n","                in_attn = out.reshape(batch_size, channels, h * w)\n","                in_attn = self.cross_attention_norms[i](in_attn)\n","                in_attn = in_attn.transpose(1, 2)\n","                assert len(context.shape) == 3, \\\n","                    \"Context shape does not match B,_,CONTEXT_DIM\"\n","                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim,\\\n","                    \"Context shape does not match B,_,CONTEXT_DIM\"\n","                context_proj = self.context_proj[i](context)\n","                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n","                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n","                out = out + out_attn\n","\n","        return out"],"metadata":{"id":"FkALS3ijuvmt","executionInfo":{"status":"ok","timestamp":1720503400475,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ishita","userId":"07336242852543837225"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","#VQVAE\n","\n","class VQVAE(nn.Module):\n","    def __init__(self, im_channels, model_config):\n","        super().__init__()\n","        self.down_channels = model_config['down_channels']\n","        self.mid_channels = model_config['mid_channels']\n","        self.down_sample = model_config['down_sample']\n","        self.num_down_layers = model_config['num_down_layers']\n","        self.num_mid_layers = model_config['num_mid_layers']\n","        self.num_up_layers = model_config['num_up_layers']\n","\n","        # To disable attention in Downblock of Encoder and Upblock of Decoder\n","        self.attns = model_config['attn_down']\n","\n","        # Latent Dimension\n","        self.z_channels = model_config['z_channels']\n","        self.codebook_size = model_config['codebook_size']\n","        self.norm_channels = model_config['norm_channels']\n","        self.num_heads = model_config['num_heads']\n","\n","        # Assertion to validate the channel information\n","        assert self.mid_channels[0] == self.down_channels[-1]\n","        assert self.mid_channels[-1] == self.down_channels[-1]\n","        assert len(self.down_sample) == len(self.down_channels) - 1\n","        assert len(self.attns) == len(self.down_channels) - 1\n","\n","        self.up_sample = list(reversed(self.down_sample))\n","\n","        # Encoder\n","        self.encoder_conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1))\n","\n","        self.encoder_layers = nn.ModuleList([])\n","        for i in range(len(self.down_channels) - 1):\n","            self.encoder_layers.append(DownBlock(self.down_channels[i], self.down_channels[i + 1],\n","                                                 t_emb_dim=None, down_sample=self.down_sample[i],\n","                                                 num_heads=self.num_heads,\n","                                                 num_layers=self.num_down_layers,\n","                                                 attn=self.attns[i],\n","                                                 norm_channels=self.norm_channels))\n","\n","        self.encoder_mids = nn.ModuleList([])\n","        for i in range(len(self.mid_channels) - 1):\n","            self.encoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1],\n","                                              t_emb_dim=None,\n","                                              num_heads=self.num_heads,\n","                                              num_layers=self.num_mid_layers,\n","                                              norm_channels=self.norm_channels))\n","\n","        self.encoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[-1])\n","        self.encoder_conv_out = nn.Conv2d(self.down_channels[-1], self.z_channels, kernel_size=3, padding=1)\n","\n","\n","        self.pre_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size=1)\n","\n","\n","        self.embedding = nn.Embedding(self.codebook_size, self.z_channels)\n","\n","\n","        #Decoder\n","        # Post Quantization Convolution\n","        self.post_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size=1)\n","        self.decoder_conv_in = nn.Conv2d(self.z_channels, self.mid_channels[-1], kernel_size=3, padding=(1, 1))\n","\n","\n","        self.decoder_mids = nn.ModuleList([])\n","        for i in reversed(range(1, len(self.mid_channels))):\n","            self.decoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i - 1],\n","                                              t_emb_dim=None,\n","                                              num_heads=self.num_heads,\n","                                              num_layers=self.num_mid_layers,\n","                                              norm_channels=self.norm_channels))\n","\n","        self.decoder_layers = nn.ModuleList([])\n","        for i in reversed(range(1, len(self.down_channels))):\n","            self.decoder_layers.append(UpBlock(self.down_channels[i], self.down_channels[i - 1],\n","                                               t_emb_dim=None, up_sample=self.down_sample[i - 1],\n","                                               num_heads=self.num_heads,\n","                                               num_layers=self.num_up_layers,\n","                                               attn=self.attns[i-1],\n","                                               norm_channels=self.norm_channels))\n","\n","        self.decoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[0])\n","        self.decoder_conv_out = nn.Conv2d(self.down_channels[0], im_channels, kernel_size=3, padding=1)\n","\n","    def quantize(self, x):\n","        B, C, H, W = x.shape\n","\n","        # B, C, H, W -> B, H, W, C\n","        x = x.permute(0, 2, 3, 1)\n","\n","        # B, H, W, C -> B, H*W, C\n","        x = x.reshape(x.size(0), -1, x.size(-1))\n","\n","\n","        # dist between (B, H*W, C) and (B, K, C) -> (B, H*W, K)\n","        dist = torch.cdist(x, self.embedding.weight[None, :].repeat((x.size(0), 1, 1)))\n","        # (B, H*W)\n","        min_encoding_indices = torch.argmin(dist, dim=-1)\n","\n","        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1))\n","\n","        # x -> B*H*W, C\n","        x = x.reshape((-1, x.size(-1)))\n","        commmitment_loss = torch.mean((quant_out.detach() - x) ** 2)\n","        codebook_loss = torch.mean((quant_out - x.detach()) ** 2)\n","        quantize_losses = {\n","            'codebook_loss': codebook_loss,\n","            'commitment_loss': commmitment_loss\n","        }\n","\n","        quant_out = x + (quant_out - x).detach()\n","\n","        # quant_out -> B, C, H, W\n","        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2)\n","        min_encoding_indices = min_encoding_indices.reshape((-1, quant_out.size(-2), quant_out.size(-1)))\n","        return quant_out, quantize_losses, min_encoding_indices\n","\n","    def encode(self, x):\n","        out = self.encoder_conv_in(x)\n","        for idx, down in enumerate(self.encoder_layers):\n","            out = down(out)\n","        for mid in self.encoder_mids:\n","            out = mid(out)\n","        out = self.encoder_norm_out(out)\n","        out = nn.SiLU()(out)\n","        out = self.encoder_conv_out(out)\n","        out = self.pre_quant_conv(out)\n","        out, quant_losses, _ = self.quantize(out)\n","        return out, quant_losses\n","\n","    def decode(self, z):\n","        out = z\n","        out = self.post_quant_conv(out)\n","        out = self.decoder_conv_in(out)\n","        for mid in self.decoder_mids:\n","            out = mid(out)\n","        for idx, up in enumerate(self.decoder_layers):\n","            out = up(out)\n","\n","        out = self.decoder_norm_out(out)\n","        out = nn.SiLU()(out)\n","        out = self.decoder_conv_out(out)\n","        return out\n","\n","    def forward(self, x):\n","        z, quant_losses = self.encode(x)\n","        out = self.decode(z)\n","        return out, z, quant_losses"],"metadata":{"id":"x1mYhkKQwkD0","executionInfo":{"status":"ok","timestamp":1720503400475,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ishita","userId":"07336242852543837225"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#Discriminator\n","\n","\n","\n","class Discriminator(nn.Module):\n","\n","    def __init__(self, im_channels=3,\n","                 conv_channels=[64, 128, 256],\n","                 kernels=[4,4,4,4],\n","                 strides=[2,2,2,1],\n","                 paddings=[1,1,1,1]):\n","        super().__init__()\n","        self.im_channels = im_channels\n","        activation = nn.LeakyReLU(0.2)\n","        layers_dim = [self.im_channels] + conv_channels + [1]\n","        self.layers = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Conv2d(layers_dim[i], layers_dim[i + 1],\n","                          kernel_size=kernels[i],\n","                          stride=strides[i],\n","                          padding=paddings[i],\n","                          bias=False if i !=0 else True),\n","                nn.BatchNorm2d(layers_dim[i + 1]) if i != len(layers_dim) - 2 and i != 0 else nn.Identity(),\n","                activation if i != len(layers_dim) - 2 else nn.Identity()\n","            )\n","            for i in range(len(layers_dim) - 1)\n","        ])\n","\n","    def forward(self, x):\n","        out = x\n","        for layer in self.layers:\n","            out = layer(out)\n","        return out\n","\n","\n","if __name__ == '__main__':\n","    x = torch.randn((2,3, 256, 256))\n","    prob = Discriminator(im_channels=3)(x)\n","    print(prob.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ufGXyPxw9G1","executionInfo":{"status":"ok","timestamp":1720503400475,"user_tz":-330,"elapsed":7,"user":{"displayName":"Ishita","userId":"07336242852543837225"}},"outputId":"f2454e88-f90f-4e18-aab3-b6408828acc6"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 1, 31, 31])\n"]}]},{"cell_type":"code","source":["import glob\n","import os\n","import random\n","import torch\n","import torchvision\n","import numpy as np\n","from PIL import Image\n","\n","from tqdm import tqdm\n","from torch.utils.data.dataset import Dataset\n","\n","\n","def load_latents(latent_path):\n","\n","    latent_maps = {}\n","    for fname in glob.glob(os.path.join(latent_path, '*.pkl')):\n","        s = pickle.load(open(fname, 'rb'))\n","        for k, v in s.items():\n","            latent_maps[k] = v[0]\n","    return latent_maps\n","class CelebDataset(Dataset):\n","\n","\n","    def __init__(self, split, im_path, im_size=256, im_channels=3, im_ext='jpg',\n","                 use_latents=False, latent_path=None, condition_config=None):\n","        self.split = split\n","        self.im_size = im_size\n","        self.im_channels = im_channels\n","        self.im_ext = im_ext\n","        self.im_path = im_path\n","        self.latent_maps = None\n","        self.use_latents = False\n","\n","        self.condition_types = [] if condition_config is None else condition_config['condition_types']\n","\n","        self.idx_to_cls_map = {}\n","        self.cls_to_idx_map ={}\n","\n","        if 'image' in self.condition_types:\n","            self.mask_channels = condition_config['image_condition_config']['image_condition_input_channels']\n","            self.mask_h = condition_config['image_condition_config']['image_condition_h']\n","            self.mask_w = condition_config['image_condition_config']['image_condition_w']\n","\n","        self.images, self.texts, self.masks = self.load_images(im_path)\n","\n","        # Whether to load images or to load latents\n","        if use_latents and latent_path is not None:\n","            latent_maps = load_latents(latent_path)\n","            if len(latent_maps) == len(self.images):\n","                self.use_latents = True\n","                self.latent_maps = latent_maps\n","                print('Found {} latents'.format(len(self.latent_maps)))\n","            else:\n","                print('Latents not found')\n","\n","    def load_images(self, im_path):\n","\n","        assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n","        ims = []\n","        fnames = glob.glob(os.path.join(im_path, 'CelebA-HQ-img/*.{}'.format('png')))\n","        fnames += glob.glob(os.path.join(im_path, 'CelebA-HQ-img/*.{}'.format('jpg')))\n","        fnames += glob.glob(os.path.join(im_path, 'CelebA-HQ-img/*.{}'.format('jpeg')))\n","        texts = []\n","        masks = []\n","\n","        if 'image' in self.condition_types:\n","            label_list = ['skin', 'nose', 'eye_g', 'l_eye', 'r_eye', 'l_brow', 'r_brow', 'l_ear', 'r_ear', 'mouth',\n","                          'u_lip', 'l_lip', 'hair', 'hat', 'ear_r', 'neck_l', 'neck', 'cloth']\n","            self.idx_to_cls_map = {idx: label_list[idx] for idx in range(len(label_list))}\n","            self.cls_to_idx_map = {label_list[idx]: idx for idx in range(len(label_list))}\n","\n","        for fname in tqdm(fnames):\n","            ims.append(fname)\n","\n","            if 'text' in self.condition_types:\n","                im_name = os.path.split(fname)[1].split('.')[0]\n","                captions_im = []\n","                with open(os.path.join(im_path, 'celeba-caption/{}.txt'.format(im_name))) as f:\n","                    for line in f.readlines():\n","                        captions_im.append(line.strip())\n","                texts.append(captions_im)\n","\n","            if 'image' in self.condition_types:\n","                im_name = int(os.path.split(fname)[1].split('.')[0])\n","                masks.append(os.path.join(im_path, 'CelebAMask-HQ-mask', '{}.png'.format(im_name)))\n","        if 'text' in self.condition_types:\n","            assert len(texts) == len(ims), \"Condition Type Text but could not find captions for all images\"\n","        if 'image' in self.condition_types:\n","            assert len(masks) == len(ims), \"Condition Type Image but could not find masks for all images\"\n","        print('Found {} images'.format(len(ims)))\n","        print('Found {} masks'.format(len(masks)))\n","        print('Found {} captions'.format(len(texts)))\n","        return ims, texts, masks\n","\n","    def get_mask(self, index):\n","\n","        mask_im = Image.open(self.masks[index])\n","        mask_im = np.array(mask_im)\n","        im_base = np.zeros((self.mask_h, self.mask_w, self.mask_channels))\n","        for orig_idx in range(len(self.idx_to_cls_map)):\n","            im_base[mask_im == (orig_idx+1), orig_idx] = 1\n","        mask = torch.from_numpy(im_base).permute(2, 0, 1).float()\n","        return mask\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, index):\n","\n","        cond_inputs = {}\n","        if 'text' in self.condition_types:\n","            cond_inputs['text'] = random.sample(self.texts[index], k=1)[0]\n","        if 'image' in self.condition_types:\n","            mask = self.get_mask(index)\n","            cond_inputs['image'] = mask\n","\n","        if self.use_latents:\n","            latent = self.latent_maps[self.images[index]]\n","            if len(self.condition_types) == 0:\n","                return latent\n","            else:\n","                return latent, cond_inputs\n","        else:\n","            im = Image.open(self.images[index])\n","            im_tensor = torchvision.transforms.Compose([\n","                torchvision.transforms.Resize(self.im_size),\n","                torchvision.transforms.CenterCrop(self.im_size),\n","                torchvision.transforms.ToTensor(),\n","            ])(im)\n","            im.close()\n","\n","\n","            im_tensor = (2 * im_tensor) - 1\n","            if len(self.condition_types) == 0:\n","                return im_tensor\n","            else:\n","                return im_tensor, cond_inputs"],"metadata":{"id":"SDOongRi3Bam","executionInfo":{"status":"ok","timestamp":1720503400475,"user_tz":-330,"elapsed":6,"user":{"displayName":"Ishita","userId":"07336242852543837225"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import glob\n","import os\n","import torchvision\n","from PIL import Image\n","from tqdm import tqdm\n","from torch.utils.data.dataset import Dataset\n","\n","\n","def load_latents(latent_path):\n","\n","    latent_maps = {}\n","    for fname in glob.glob(os.path.join(latent_path, '*.pkl')):\n","        s = pickle.load(open(fname, 'rb'))\n","        for k, v in s.items():\n","            latent_maps[k] = v[0]\n","    return latent_maps\n","class MnistDataset(Dataset):\n","\n","    def __init__(self, split, im_path, im_size, im_channels,\n","                 use_latents=False, latent_path=None, condition_config=None):\n","\n","        self.split = split\n","        self.im_size = im_size\n","        self.im_channels = im_channels\n","\n","\n","        self.latent_maps = None\n","        self.use_latents = False\n","\n","        # Conditioning for the dataset\n","        self.condition_types = [] if condition_config is None else condition_config['condition_types']\n","\n","        self.images, self.labels = self.load_images(im_path)\n","\n","\n","        if use_latents and latent_path is not None:\n","            latent_maps = load_latents(latent_path)\n","            if len(latent_maps) == len(self.images):\n","                self.use_latents = True\n","                self.latent_maps = latent_maps\n","                print('Found {} latents'.format(len(self.latent_maps)))\n","            else:\n","                print('Latents not found')\n","\n","    def load_images(self, im_path):\n","\n","        assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n","        ims = []\n","        labels = []\n","        for d_name in tqdm(os.listdir(im_path)):\n","            fnames = glob.glob(os.path.join(im_path, d_name, '*.{}'.format('png')))\n","            fnames += glob.glob(os.path.join(im_path, d_name, '*.{}'.format('jpg')))\n","            fnames += glob.glob(os.path.join(im_path, d_name, '*.{}'.format('jpeg')))\n","            for fname in fnames:\n","                ims.append(fname)\n","                if 'class' in self.condition_types:\n","                    labels.append(int(d_name))\n","        print('Found {} images for split {}'.format(len(ims), self.split))\n","        return ims, labels\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, index):\n","\n","        cond_inputs = {}\n","        if 'class' in self.condition_types:\n","            cond_inputs['class'] = self.labels[index]\n","\n","\n","        if self.use_latents:\n","            latent = self.latent_maps[self.images[index]]\n","            if len(self.condition_types) == 0:\n","                return latent\n","            else:\n","                return latent, cond_inputs\n","        else:\n","            im = Image.open(self.images[index])\n","            im_tensor = torchvision.transforms.ToTensor()(im)\n","\n","\n","            im_tensor = (2 * im_tensor) - 1\n","            if len(self.condition_types) == 0:\n","                return im_tensor\n","            else:\n","                return im_tensor, cond_inputs\n",""],"metadata":{"id":"JJhKQXJb3B8z","executionInfo":{"status":"ok","timestamp":1720503400475,"user_tz":-330,"elapsed":6,"user":{"displayName":"Ishita","userId":"07336242852543837225"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import yaml\n","import argparse\n","import torch\n","import random\n","import torchvision\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","from torch.utils.data.dataloader import DataLoader\n","\n","from torch.optim import Adam\n","from torchvision.utils import make_grid\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","#Autoencoder training\n","def train(args):\n","\n","    with open(args.config_path, 'r') as file:\n","        try:\n","            config = yaml.safe_load(file)\n","        except yaml.YAMLError as exc:\n","            print(exc)\n","    print(config)\n","\n","    dataset_config = config['dataset_params']\n","    autoencoder_config = config['autoencoder_params']\n","    train_config = config['train_params']\n","\n","\n","    seed = train_config['seed']\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    if device == 'cuda':\n","        torch.cuda.manual_seed_all(seed)\n","\n","    # Create the model and dataset #\n","    model = VQVAE(im_channels=dataset_config['im_channels'],\n","                  model_config=autoencoder_config).to(device)\n","\n","    im_dataset_cls = {\n","        'mnist': MnistDataset,\n","        'celebhq': CelebDataset,\n","    }.get(dataset_config['name'])\n","\n","    im_dataset = im_dataset_cls(split='train',\n","                                im_path=dataset_config['im_path'],\n","                                im_size=dataset_config['im_size'],\n","                                im_channels=dataset_config['im_channels'])\n","\n","    data_loader = DataLoader(im_dataset,\n","                             batch_size=train_config['autoencoder_batch_size'],\n","                             shuffle=True)\n","\n","\n","    if not os.path.exists(train_config['task_name']):\n","        os.mkdir(train_config['task_name'])\n","\n","    num_epochs = train_config['autoencoder_epochs']\n","\n","\n","    recon_criterion = torch.nn.MSELoss()\n","\n","    disc_criterion = torch.nn.MSELoss()\n","\n","    lpips_model = LPIPS().eval().to(device)\n","    discriminator = Discriminator(im_channels=dataset_config['im_channels']).to(device)\n","\n","    optimizer_d = Adam(discriminator.parameters(), lr=train_config['autoencoder_lr'], betas=(0.5, 0.999))\n","    optimizer_g = Adam(model.parameters(), lr=train_config['autoencoder_lr'], betas=(0.5, 0.999))\n","\n","    disc_step_start = train_config['disc_start']\n","    step_count = 0\n","\n","    acc_steps = train_config['autoencoder_acc_steps']\n","    image_save_steps = train_config['autoencoder_img_save_steps']\n","    img_save_count = 0\n","\n","    for epoch_idx in range(num_epochs):\n","        recon_losses = []\n","        codebook_losses = []\n","\n","        perceptual_losses = []\n","        disc_losses = []\n","        gen_losses = []\n","        losses = []\n","\n","        optimizer_g.zero_grad()\n","        optimizer_d.zero_grad()\n","\n","        for im in tqdm(data_loader):\n","            step_count += 1\n","            im = im.float().to(device)\n","\n","\n","            model_output = model(im)\n","            output, z, quantize_losses = model_output\n","\n","\n","            if step_count % image_save_steps == 0 or step_count == 1:\n","                sample_size = min(8, im.shape[0])\n","                save_output = torch.clamp(output[:sample_size], -1., 1.).detach().cpu()\n","                save_output = ((save_output + 1) / 2)\n","                save_input = ((im[:sample_size] + 1) / 2).detach().cpu()\n","\n","                grid = make_grid(torch.cat([save_input, save_output], dim=0), nrow=sample_size)\n","                img = torchvision.transforms.ToPILImage()(grid)\n","                if not os.path.exists(os.path.join(train_config['task_name'],'vqvae_autoencoder_samples')):\n","                    os.mkdir(os.path.join(train_config['task_name'], 'vqvae_autoencoder_samples'))\n","                img.save(os.path.join(train_config['task_name'],'vqvae_autoencoder_samples',\n","                                      'current_autoencoder_sample_{}.png'.format(img_save_count)))\n","                img_save_count += 1\n","                img.close()\n","\n","\n","            recon_loss = recon_criterion(output, im)\n","            recon_losses.append(recon_loss.item())\n","            recon_loss = recon_loss / acc_steps\n","            g_loss = (recon_loss +\n","                      (train_config['codebook_weight'] * quantize_losses['codebook_loss'] / acc_steps) +\n","                      (train_config['commitment_beta'] * quantize_losses['commitment_loss'] / acc_steps))\n","            codebook_losses.append(train_config['codebook_weight'] * quantize_losses['codebook_loss'].item())\n","\n","            if step_count > disc_step_start:\n","                disc_fake_pred = discriminator(model_output[0])\n","                disc_fake_loss = disc_criterion(disc_fake_pred,\n","                                                torch.ones(disc_fake_pred.shape,\n","                                                           device=disc_fake_pred.device))\n","                gen_losses.append(train_config['disc_weight'] * disc_fake_loss.item())\n","                g_loss += train_config['disc_weight'] * disc_fake_loss / acc_steps\n","            lpips_loss = torch.mean(lpips_model(output, im)) / acc_steps\n","            perceptual_losses.append(train_config['perceptual_weight'] * lpips_loss.item())\n","            g_loss += train_config['perceptual_weight']*lpips_loss / acc_steps\n","            losses.append(g_loss.item())\n","            g_loss.backward()\n","\n","            if step_count > disc_step_start:\n","                fake = output\n","                disc_fake_pred = discriminator(fake.detach())\n","                disc_real_pred = discriminator(im)\n","                disc_fake_loss = disc_criterion(disc_fake_pred,\n","                                                torch.zeros(disc_fake_pred.shape,\n","                                                            device=disc_fake_pred.device))\n","                disc_real_loss = disc_criterion(disc_real_pred,\n","                                                torch.ones(disc_real_pred.shape,\n","                                                           device=disc_real_pred.device))\n","                disc_loss = train_config['disc_weight'] * (disc_fake_loss + disc_real_loss) / 2\n","                disc_losses.append(disc_loss.item())\n","                disc_loss = disc_loss / acc_steps\n","                disc_loss.backward()\n","                if step_count % acc_steps == 0:\n","                    optimizer_d.step()\n","                    optimizer_d.zero_grad()\n","\n","            if step_count % acc_steps == 0:\n","                optimizer_g.step()\n","                optimizer_g.zero_grad()\n","        optimizer_d.step()\n","        optimizer_d.zero_grad()\n","        optimizer_g.step()\n","        optimizer_g.zero_grad()\n","        if len(disc_losses) > 0:\n","            print(\n","                'Finished epoch: {} | Recon Loss : {:.4f} | Perceptual Loss : {:.4f} | '\n","                'Codebook : {:.4f} | G Loss : {:.4f} | D Loss {:.4f}'.\n","                format(epoch_idx + 1,\n","                       np.mean(recon_losses),\n","                       np.mean(perceptual_losses),\n","                       np.mean(codebook_losses),\n","                       np.mean(gen_losses),\n","                       np.mean(disc_losses)))\n","        else:\n","            print('Finished epoch: {} | Recon Loss : {:.4f} | Perceptual Loss : {:.4f} | Codebook : {:.4f}'.\n","                  format(epoch_idx + 1,\n","                         np.mean(recon_losses),\n","                         np.mean(perceptual_losses),\n","                         np.mean(codebook_losses)))\n","\n","        torch.save(model.state_dict(), os.path.join(train_config['task_name'],\n","                                                    train_config['vqvae_autoencoder_ckpt_name']))\n","        torch.save(discriminator.state_dict(), os.path.join(train_config['task_name'],\n","                                                            train_config['vqvae_discriminator_ckpt_name']))\n","    print('Done Training...')\n","\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description='Arguments for vq vae training')\n","    parser.add_argument('--config', dest='config_path',\n","                        default='mnist.yaml', type=str)\n","    args, unknown = parser.parse_known_args()\n","    train(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AiYFbbbJ1kWs","executionInfo":{"status":"ok","timestamp":1720504474254,"user_tz":-330,"elapsed":904257,"user":{"displayName":"Ishita","userId":"07336242852543837225"}},"outputId":"ab22a76e-bced-4ba9-faed-d90ed007382c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["{'dataset_params': {'im_path': 'data/train/images', 'im_channels': 1, 'im_size': 28, 'name': 'mnist'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0015, 'beta_end': 0.0195}, 'ldm_params': {'down_channels': [128, 256, 256, 256], 'mid_channels': [256, 256], 'down_sample': [False, False, False], 'attn_down': [True, True, True], 'time_emb_dim': 256, 'norm_channels': 32, 'num_heads': 16, 'conv_out_channels': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2}, 'autoencoder_params': {'z_channels': 3, 'codebook_size': 20, 'down_channels': [32, 64, 128], 'mid_channels': [128, 128], 'down_sample': [True, True], 'attn_down': [False, False], 'norm_channels': 32, 'num_heads': 16, 'num_down_layers': 1, 'num_mid_layers': 1, 'num_up_layers': 1}, 'train_params': {'seed': 1111, 'task_name': 'mnist', 'ldm_batch_size': 64, 'autoencoder_batch_size': 64, 'disc_start': 1000, 'disc_weight': 0.5, 'codebook_weight': 1, 'commitment_beta': 0.2, 'perceptual_weight': 1, 'kl_weight': 5e-06, 'ldm_epochs': 100, 'autoencoder_epochs': 10, 'num_samples': 25, 'num_grid_rows': 5, 'ldm_lr': 1e-05, 'autoencoder_lr': 0.0001, 'autoencoder_acc_steps': 1, 'autoencoder_img_save_steps': 8, 'save_latents': False, 'vae_latent_dir_name': 'vae_latents', 'vqvae_latent_dir_name': 'vqvae_latents', 'ldm_ckpt_name': 'ddpm_ckpt.pth', 'vqvae_autoencoder_ckpt_name': 'vqvae_autoencoder_ckpt.pth', 'vae_autoencoder_ckpt_name': 'vae_autoencoder_ckpt.pth', 'vqvae_discriminator_ckpt_name': 'vqvae_discriminator_ckpt.pth', 'vae_discriminator_ckpt_name': 'vae_discriminator_ckpt.pth'}}\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 41.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Found 60000 images for split train\n","Loading model from: /content/vgg.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 937/938 [01:12<00:00, 13.16it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return F.conv2d(input, weight, bias, self.stride,\n","100%|██████████| 938/938 [01:12<00:00, 12.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Finished epoch: 1 | Recon Loss : 0.1022 | Perceptual Loss : 0.1251 | Codebook : 0.1062\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 938/938 [01:31<00:00, 10.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Finished epoch: 2 | Recon Loss : 0.0399 | Perceptual Loss : 0.0549 | Codebook : 0.1005 | G Loss : 0.0358 | D Loss 0.1551\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 938/938 [01:33<00:00, 10.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Finished epoch: 3 | Recon Loss : 0.0304 | Perceptual Loss : 0.0380 | Codebook : 0.0830 | G Loss : 0.0335 | D Loss 0.1544\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 938/938 [01:32<00:00, 10.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Finished epoch: 4 | Recon Loss : 0.0244 | Perceptual Loss : 0.0285 | Codebook : 0.0700 | G Loss : 0.0325 | D Loss 0.1555\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 938/938 [01:31<00:00, 10.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Finished epoch: 5 | Recon Loss : 0.0212 | Perceptual Loss : 0.0238 | Codebook : 0.0625 | G Loss : 0.0322 | D Loss 0.1558\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 938/938 [01:32<00:00, 10.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Finished epoch: 6 | Recon Loss : 0.0192 | Perceptual Loss : 0.0213 | Codebook : 0.0588 | G Loss : 0.0321 | D Loss 0.1558\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 938/938 [01:31<00:00, 10.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Finished epoch: 7 | Recon Loss : 0.0180 | Perceptual Loss : 0.0197 | Codebook : 0.0565 | G Loss : 0.0320 | D Loss 0.1558\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 938/938 [01:31<00:00, 10.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Finished epoch: 8 | Recon Loss : 0.0170 | Perceptual Loss : 0.0185 | Codebook : 0.0553 | G Loss : 0.0320 | D Loss 0.1558\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 938/938 [01:32<00:00, 10.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Finished epoch: 9 | Recon Loss : 0.0162 | Perceptual Loss : 0.0176 | Codebook : 0.0543 | G Loss : 0.0320 | D Loss 0.1558\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 938/938 [01:31<00:00, 10.24it/s]"]},{"output_type":"stream","name":"stdout","text":["Finished epoch: 10 | Recon Loss : 0.0156 | Perceptual Loss : 0.0168 | Codebook : 0.0536 | G Loss : 0.0319 | D Loss 0.1558\n","Done Training...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["\n","\n","class Unet(nn.Module):\n","\n","\n","    def __init__(self, im_channels, model_config):\n","        super().__init__()\n","        self.down_channels = model_config['down_channels']\n","        self.mid_channels = model_config['mid_channels']\n","        self.t_emb_dim = model_config['time_emb_dim']\n","        self.down_sample = model_config['down_sample']\n","        self.num_down_layers = model_config['num_down_layers']\n","        self.num_mid_layers = model_config['num_mid_layers']\n","        self.num_up_layers = model_config['num_up_layers']\n","        self.attns = model_config['attn_down']\n","        self.norm_channels = model_config['norm_channels']\n","        self.num_heads = model_config['num_heads']\n","        self.conv_out_channels = model_config['conv_out_channels']\n","\n","        assert self.mid_channels[0] == self.down_channels[-1]\n","        assert self.mid_channels[-1] == self.down_channels[-2]\n","        assert len(self.down_sample) == len(self.down_channels) - 1\n","        assert len(self.attns) == len(self.down_channels) - 1\n","\n","        self.t_proj = nn.Sequential(\n","            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n","            nn.SiLU(),\n","            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n","        )\n","\n","        self.up_sample = list(reversed(self.down_sample))\n","        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=1)\n","\n","        self.downs = nn.ModuleList([])\n","        for i in range(len(self.down_channels) - 1):\n","            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i + 1], self.t_emb_dim,\n","                                        down_sample=self.down_sample[i],\n","                                        num_heads=self.num_heads,\n","                                        num_layers=self.num_down_layers,\n","                                        attn=self.attns[i], norm_channels=self.norm_channels))\n","\n","        self.mids = nn.ModuleList([])\n","        for i in range(len(self.mid_channels) - 1):\n","            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1], self.t_emb_dim,\n","                                      num_heads=self.num_heads,\n","                                      num_layers=self.num_mid_layers,\n","                                      norm_channels=self.norm_channels))\n","\n","        self.ups = nn.ModuleList([])\n","        for i in reversed(range(len(self.down_channels) - 1)):\n","            self.ups.append(UpBlockUnet(self.down_channels[i] * 2, self.down_channels[i - 1] if i != 0 else self.conv_out_channels,\n","                                    self.t_emb_dim, up_sample=self.down_sample[i],\n","                                        num_heads=self.num_heads,\n","                                        num_layers=self.num_up_layers,\n","                                        norm_channels=self.norm_channels))\n","\n","        self.norm_out = nn.GroupNorm(self.norm_channels, self.conv_out_channels)\n","        self.conv_out = nn.Conv2d(self.conv_out_channels, im_channels, kernel_size=3, padding=1)\n","\n","    def forward(self, x, t):\n","\n","        # B x C x H x W\n","        out = self.conv_in(x)\n","        # B x C1 x H x W\n","\n","        # t_emb -> B x t_emb_dim\n","        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n","        t_emb = self.t_proj(t_emb)\n","\n","        down_outs = []\n","\n","        for idx, down in enumerate(self.downs):\n","            down_outs.append(out)\n","            out = down(out, t_emb)\n","        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n","        # out B x C4 x H/4 x W/4\n","\n","        for mid in self.mids:\n","            out = mid(out, t_emb)\n","        # out B x C3 x H/4 x W/4\n","\n","        for up in self.ups:\n","            down_out = down_outs.pop()\n","            out = up(out, down_out, t_emb)\n","            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n","        out = self.norm_out(out)\n","        out = nn.SiLU()(out)\n","        out = self.conv_out(out)\n","        # out B x C x H x W\n","        return out"],"metadata":{"id":"V70FMFXaFbQX","executionInfo":{"status":"ok","timestamp":1720504930982,"user_tz":-330,"elapsed":651,"user":{"displayName":"Ishita","userId":"07336242852543837225"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","\n","class LinearNoiseScheduler:\n","\n","    def __init__(self, num_timesteps, beta_start, beta_end):\n","        self.num_timesteps = num_timesteps\n","        self.beta_start = beta_start\n","        self.beta_end = beta_end\n","\n","        self.betas = (\n","                torch.linspace(beta_start ** 0.5, beta_end ** 0.5, num_timesteps) ** 2\n","        )\n","        self.alphas = 1. - self.betas\n","        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n","        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n","        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n","\n","    def add_noise(self, original, noise, t):\n","\n","\n","        original_shape = original.shape\n","        batch_size = original_shape[0]\n","\n","        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n","        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n","\n","        # Reshape till (B,) becomes (B,1,1,1) if image is (B,C,H,W)\n","        for _ in range(len(original_shape) - 1):\n","            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n","        for _ in range(len(original_shape) - 1):\n","            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n","\n","        # Apply and Return Forward process equation\n","        return (sqrt_alpha_cum_prod.to(original.device) * original\n","                + sqrt_one_minus_alpha_cum_prod.to(original.device) * noise)\n","\n","    def sample_prev_timestep(self, xt, noise_pred, t):\n","\n","        x0 = ((xt - (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t] * noise_pred)) /\n","              torch.sqrt(self.alpha_cum_prod.to(xt.device)[t]))\n","        x0 = torch.clamp(x0, -1., 1.)\n","\n","        mean = xt - ((self.betas.to(xt.device)[t]) * noise_pred) / (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t])\n","        mean = mean / torch.sqrt(self.alphas.to(xt.device)[t])\n","\n","        if t == 0:\n","            return mean, x0\n","        else:\n","            variance = (1 - self.alpha_cum_prod.to(xt.device)[t - 1]) / (1.0 - self.alpha_cum_prod.to(xt.device)[t])\n","            variance = variance * self.betas.to(xt.device)[t]\n","            sigma = variance ** 0.5\n","            z = torch.randn(xt.shape).to(xt.device)\n","\n","\n","            return mean + sigma * z, x0"],"metadata":{"id":"raHBJhWmF5_w","executionInfo":{"status":"ok","timestamp":1720505054792,"user_tz":-330,"elapsed":1102,"user":{"displayName":"Ishita","userId":"07336242852543837225"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["import torch\n","import yaml\n","import argparse\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","from torch.optim import Adam\n","\n","from torch.utils.data import DataLoader\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","#training of latent diffusion model\n","def train(args):\n","\n","    with open(args.config_path, 'r') as file:\n","        try:\n","            config = yaml.safe_load(file)\n","        except yaml.YAMLError as exc:\n","            print(exc)\n","    print(config)\n","\n","    diffusion_config = config['diffusion_params']\n","    dataset_config = config['dataset_params']\n","    diffusion_model_config = config['ldm_params']\n","    autoencoder_model_config = config['autoencoder_params']\n","    train_config = config['train_params']\n","\n","    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n","                                     beta_start=diffusion_config['beta_start'],\n","                                     beta_end=diffusion_config['beta_end'])\n","\n","    im_dataset_cls = {\n","        'mnist': MnistDataset,\n","        'celebhq': CelebDataset,\n","    }.get(dataset_config['name'])\n","\n","    im_dataset = im_dataset_cls(split='train',\n","                                im_path=dataset_config['im_path'],\n","                                im_size=dataset_config['im_size'],\n","                                im_channels=dataset_config['im_channels'],\n","                                use_latents=True,\n","                                latent_path=os.path.join(train_config['task_name'],\n","                                                         train_config['vqvae_latent_dir_name'])\n","                                )\n","\n","    data_loader = DataLoader(im_dataset,\n","                             batch_size=train_config['ldm_batch_size'],\n","                             shuffle=True)\n","\n","    model = Unet(im_channels=autoencoder_model_config['z_channels'],\n","                 model_config=diffusion_model_config).to(device)\n","    model.train()\n","\n","    if not im_dataset.use_latents:\n","        print('Loading vqvae model as latents not present')\n","        vae = VQVAE(im_channels=dataset_config['im_channels'],\n","                    model_config=autoencoder_model_config).to(device)\n","        vae.eval()\n","        if os.path.exists(os.path.join(train_config['task_name'],\n","                                       train_config['vqvae_autoencoder_ckpt_name'])):\n","            print('Loaded vae checkpoint')\n","            vae.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n","                                                        train_config['vqvae_autoencoder_ckpt_name']),\n","                                           map_location=device))\n","    num_epochs = train_config['ldm_epochs']\n","    optimizer = Adam(model.parameters(), lr=train_config['ldm_lr'])\n","    criterion = torch.nn.MSELoss()\n","\n","    if not im_dataset.use_latents:\n","        for param in vae.parameters():\n","            param.requires_grad = False\n","\n","    for epoch_idx in range(num_epochs):\n","        losses = []\n","        for im in tqdm(data_loader):\n","            optimizer.zero_grad()\n","            im = im.float().to(device)\n","            if not im_dataset.use_latents:\n","                with torch.no_grad():\n","                    im, _ = vae.encode(im)\n","\n","            noise = torch.randn_like(im).to(device)\n","\n","            t = torch.randint(0, diffusion_config['num_timesteps'], (im.shape[0],)).to(device)\n","\n","            noisy_im = scheduler.add_noise(im, noise, t)\n","            noise_pred = model(noisy_im, t)\n","\n","            loss = criterion(noise_pred, noise)\n","            losses.append(loss.item())\n","            loss.backward()\n","            optimizer.step()\n","        print('Finished epoch:{} | Loss : {:.4f}'.format(\n","            epoch_idx + 1,\n","            np.mean(losses)))\n","\n","        torch.save(model.state_dict(), os.path.join(train_config['task_name'],\n","                                                    train_config['ldm_ckpt_name']))\n","\n","    print('Done Training ...')\n","\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description='Arguments for ddpm training')\n","    parser.add_argument('--config', dest='config_path',\n","                        default='mnist.yaml', type=str)\n","    args, unknown = parser.parse_known_args()\n","    train(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITCXlrXxFiZk","executionInfo":{"status":"ok","timestamp":1720506881574,"user_tz":-330,"elapsed":573906,"user":{"displayName":"Ishita","userId":"07336242852543837225"}},"outputId":"73c96d3d-f897-4ffe-bd19-5c683a246715"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["{'dataset_params': {'im_path': 'data/train/images', 'im_channels': 1, 'im_size': 28, 'name': 'mnist'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0015, 'beta_end': 0.0195}, 'ldm_params': {'down_channels': [128, 256, 256, 256], 'mid_channels': [256, 256], 'down_sample': [False, False, False], 'attn_down': [True, True, True], 'time_emb_dim': 256, 'norm_channels': 32, 'num_heads': 16, 'conv_out_channels': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2}, 'autoencoder_params': {'z_channels': 3, 'codebook_size': 20, 'down_channels': [32, 64, 128], 'mid_channels': [128, 128], 'down_sample': [True, True], 'attn_down': [False, False], 'norm_channels': 32, 'num_heads': 16, 'num_down_layers': 1, 'num_mid_layers': 1, 'num_up_layers': 1}, 'train_params': {'seed': 1111, 'task_name': 'mnist', 'ldm_batch_size': 64, 'autoencoder_batch_size': 64, 'disc_start': 1000, 'disc_weight': 0.5, 'codebook_weight': 1, 'commitment_beta': 0.2, 'perceptual_weight': 1, 'kl_weight': 5e-06, 'ldm_epochs': 5, 'autoencoder_epochs': 10, 'num_samples': 25, 'num_grid_rows': 5, 'ldm_lr': 1e-05, 'autoencoder_lr': 0.0001, 'autoencoder_acc_steps': 1, 'autoencoder_img_save_steps': 8, 'save_latents': False, 'vae_latent_dir_name': 'vae_latents', 'vqvae_latent_dir_name': 'vqvae_latents', 'ldm_ckpt_name': 'ddpm_ckpt.pth', 'vqvae_autoencoder_ckpt_name': 'vqvae_autoencoder_ckpt.pth', 'vae_autoencoder_ckpt_name': 'vae_autoencoder_ckpt.pth', 'vqvae_discriminator_ckpt_name': 'vqvae_discriminator_ckpt.pth', 'vae_discriminator_ckpt_name': 'vae_discriminator_ckpt.pth'}}\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 43.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Found 60000 images for split train\n","Latents not found\n","Loading vqvae model as latents not present\n","Loaded vae checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 938/938 [01:54<00:00,  8.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Finished epoch:1 | Loss : 0.2802\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 938/938 [01:54<00:00,  8.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Finished epoch:2 | Loss : 0.1664\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 938/938 [01:54<00:00,  8.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Finished epoch:3 | Loss : 0.1480\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 938/938 [01:54<00:00,  8.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Finished epoch:4 | Loss : 0.1384\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 938/938 [01:54<00:00,  8.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Finished epoch:5 | Loss : 0.1333\n","Done Training ...\n"]}]}]}